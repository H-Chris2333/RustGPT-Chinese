# ğŸ¦€ RustGPT-Chinese - Chinese-Only LLM

[![Check](https://github.com/H-Chris233/RustGPT-Chinese/actions/workflows/check.yml/badge.svg)](https://github.com/H-Chris233/RustGPT-Chinese/actions/workflows/check.yml) [![Test](https://github.com/H-Chris233/RustGPT-Chinese/actions/workflows/test.yml/badge.svg)](https://github.com/H-Chris233/RustGPT-Chinese/actions/workflows/test.yml)

**[ä¸­æ–‡æ–‡æ¡£](README_zh.md) | [ä¸­æ–‡æ–‡æ¡£](README_zh.md)**

A complete **Chinese-only Large Language Model implementation in pure Rust** with no external ML frameworks. Built from the ground up using only `ndarray` for matrix operations.

## ğŸš€ What This Is

This project demonstrates how to build a transformer-based language model from scratch in Rust that is specialized for Chinese language processing, including:
- **Pre-training** on Chinese factual text completion
- **Instruction tuning** for Chinese conversational AI
- **Interactive chat mode** for Chinese language testing
- **Full backpropagation** with gradient clipping
- **Modular architecture** with clean separation of concerns

## âŒ What This Isn't

This is not a production grade Chinese LLM. It is so far away from the larger Chinese models.

This is just a toy project that demonstrates how Chinese LLMs work under the hood.

## ğŸ” Key Files to Explore

Start with these two core files to understand the implementation:

- **[`src/main.rs`](src/main.rs)** - Training pipeline, data preparation, and interactive mode
- **[`src/llm.rs`](src/llm.rs)** - Core LLM implementation with forward/backward passes and training logic

## ğŸ—ï¸ Architecture

The model uses a **transformer-based architecture** with the following components:

```
Input Text â†’ Tokenization â†’ Embeddings â†’ Transformer Blocks â†’ Output Projection â†’ Predictions
```

### Project Structure

```
src/
â”œâ”€â”€ main.rs              # ğŸ¯ Training pipeline and interactive mode
â”œâ”€â”€ llm.rs               # ğŸ§  Core LLM implementation and training logic
â”œâ”€â”€ lib.rs               # ğŸ“š Library exports and constants
â”œâ”€â”€ transformer.rs       # ğŸ”„ Transformer block (attention + feed-forward)
â”œâ”€â”€ self_attention.rs    # ğŸ‘€ Multi-head self-attention mechanism
â”œâ”€â”€ feed_forward.rs      # âš¡ Position-wise feed-forward networks
â”œâ”€â”€ embeddings.rs        # ğŸ“Š Token embedding layer
â”œâ”€â”€ output_projection.rs # ğŸ° Final linear layer for vocabulary predictions
â”œâ”€â”€ vocab.rs            # ğŸ“ Vocabulary management and tokenization
â”œâ”€â”€ layer_norm.rs       # ğŸ§® Layer normalization
â””â”€â”€ adam.rs             # ğŸƒ Adam optimizer implementation

tests/
â”œâ”€â”€ llm_test.rs         # Tests for core LLM functionality
â”œâ”€â”€ transformer_test.rs # Tests for transformer blocks
â”œâ”€â”€ self_attention_test.rs # Tests for attention mechanisms
â”œâ”€â”€ feed_forward_test.rs # Tests for feed-forward layers
â”œâ”€â”€ embeddings_test.rs  # Tests for embedding layers
â”œâ”€â”€ vocab_test.rs       # Tests for vocabulary handling
â”œâ”€â”€ adam_test.rs        # Tests for optimizer
â””â”€â”€ output_projection_test.rs # Tests for output layer
```

## ğŸ§ª What The Model Learns

The implementation includes two training phases specialized for Chinese:

1. **Pre-training**: Learns Chinese world knowledge from Chinese factual statements
   - "å¤ªé˜³ä»ä¸œæ–¹å‡èµ·ï¼Œåœ¨è¥¿æ–¹è½ä¸‹"
   - "æ°´ç”±äºé‡åŠ›è€Œä»é«˜å¤„æµå‘ä½å¤„"
   - "å±±è„‰æ˜¯é«˜å¤§è€Œå¤šå²©çŸ³çš„åœ°å½¢"

2. **Instruction Tuning**: Learns Chinese conversational patterns
   - "ç”¨æˆ·ï¼šå±±è„‰æ˜¯å¦‚ä½•å½¢æˆçš„ï¼ŸåŠ©æ‰‹ï¼šå±±è„‰é€šè¿‡æ„é€ åŠ›æˆ–ç«å±±æ´»åŠ¨å½¢æˆ..."
   - Handles Chinese greetings, explanations, and follow-up questions

## ğŸš€ Quick Start

```bash
# Clone and run
git clone https://github.com/H-Chris233/RustGPT-Chinese.git
cd RustGPT-Chinese
cargo run

# The model will:
# 1. Build vocabulary from Chinese training data
# 2. Pre-train on Chinese factual statements (100 epochs)
# 3. Instruction-tune on Chinese conversational data (100 epochs)
# 4. Enter interactive mode for Chinese testing
```

## ğŸ® Interactive Mode

After training, test the model interactively in Chinese:

```
Enter prompt: å±±è„‰æ˜¯å¦‚ä½•å½¢æˆçš„?
Model output: å±±è„‰é€šè¿‡æ„é€ åŠ›æˆ–ç«å±±æ´»åŠ¨åœ¨é•¿æ—¶é—´çš„åœ°è´¨æ—¶æœŸå†…å½¢æˆ

Enter prompt: é™é›¨çš„åŸå› æ˜¯ä»€ä¹ˆ?
Model output: é™é›¨æ˜¯ç”±äº‘ä¸­çš„æ°´è’¸æ°”å‡ç»“æˆæ°´æ»´ï¼Œå½“æ°´æ»´å˜å¾—å¤ªé‡è€Œæ— æ³•æ‚¬æµ®åœ¨ç©ºæ°”ä¸­æ—¶å½¢æˆçš„
```

## ğŸ§® Technical Implementation

### Model Configuration
- **Vocabulary Size**: Dynamic (built from training data)
- **Embedding Dimension**: 128 (defined by `EMBEDDING_DIM` in `src/lib.rs`)
- **Hidden Dimension**: 256 (defined by `HIDDEN_DIM` in `src/lib.rs`)
- **Max Sequence Length**: 80 tokens (defined by `MAX_SEQ_LEN` in `src/lib.rs`)
- **Architecture**: 3 Transformer blocks + embeddings + output projection

### Training Details
- **Optimizer**: Adam with gradient clipping
- **Pre-training LR**: 0.0005 (100 epochs)
- **Instruction Tuning LR**: 0.0001 (100 epochs)
- **Loss Function**: Cross-entropy loss
- **Gradient Clipping**: L2 norm capped at 5.0

### Key Features
- **Custom tokenization** with punctuation handling
- **Greedy decoding** for text generation
- **Gradient clipping** for training stability
- **Modular layer system** with clean interfaces
- **Comprehensive test coverage** for all components

## ğŸ”§ Development

```bash
# Run all tests
cargo test

# Test specific components
cargo test --test llm_test
cargo test --test transformer_test
cargo test --test self_attention_test

# Build optimized version
cargo build --release

# Run with verbose output
cargo test -- --nocapture
```

## ğŸ§  Learning Resources

This implementation demonstrates key ML concepts for Chinese language models:
- **Transformer architecture** (attention, feed-forward, layer norm)
- **Backpropagation** through neural networks
- **Chinese language model training** (pre-training + fine-tuning)
- **Chinese tokenization** and vocabulary management
- **Gradient-based optimization** with Adam

Perfect for understanding how Chinese LLMs work under the hood!

## ğŸ“Š Dependencies

- `ndarray` - N-dimensional arrays for matrix operations
- `rand` + `rand_distr` - Random number generation for initialization

No PyTorch, TensorFlow, or Candle - just pure Rust and linear algebra!

## ğŸ¤ Contributing

Contributions are welcome! This project is perfect for learning and experimentation.

### High Priority Features Needed
- **ğŸª Model Persistence** - Save/load trained parameters to disk (currently all in-memory)
- **âš¡ Performance optimizations** - SIMD, parallel training, memory efficiency
- **ğŸ¯ Better sampling** - Beam search, top-k/top-p, temperature scaling
- **ğŸ“Š Evaluation metrics** - Perplexity, benchmarks, training visualizations

### Areas for Improvement
- **Advanced architectures** (multi-head attention, positional encoding, RoPE)
- **Training improvements** (different optimizers, learning rate schedules, regularization)
- **Chinese data handling** (larger Chinese datasets, Chinese tokenizer improvements, streaming)
- **Model analysis** (attention visualization, gradient analysis, interpretability)

### Getting Started
1. Fork the repository
2. Create a feature branch: `git checkout -b feature/model-persistence`
3. Make your changes and add tests
4. Run the test suite: `cargo test`
5. Submit a pull request with a clear description

### Code Style
- Follow standard Rust conventions (`cargo fmt`)
- Add comprehensive tests for new features
- Update documentation and README as needed
- Keep the "from scratch" philosophy - avoid heavy ML dependencies
- Focus on Chinese language processing improvements

### Ideas for Contributions
- ğŸš€ **Beginner**: Model save/load, more Chinese training data, config files
- ğŸ”¥ **Intermediate**: Better Chinese tokenization, Chinese-specific optimizations, training checkpoints
- âš¡ **Advanced**: Multi-head attention improvements for Chinese, layer parallelization, custom Chinese optimizations

Questions? Open an issue or start a discussion!

No PyTorch, TensorFlow, or Candle - just pure Rust and linear algebra!

# æ€§èƒ½ä¼˜åŒ–æ–‡æ¡£ (v0.4.0)

## æ¦‚è¿°

v0.4.0 ç‰ˆæœ¬å¼•å…¥äº†å¤šé¡¹æ€§èƒ½ä¼˜åŒ–ï¼Œç›®æ ‡æ˜¯åœ¨ä¿æŒæ¨¡å‹ç²¾åº¦çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ï¼Œå¹¶å‡å°‘å†…å­˜å ç”¨ã€‚

## ä¼˜åŒ–é¡¹ç›®

### 1. âœ… BLAS å¼ é‡è®¡ç®—åŠ é€Ÿ

**å®ç°ä½ç½®**: `Cargo.toml`

**å˜æ›´å†…å®¹**:
```toml
# BLAS æ˜¯å¯é€‰ç‰¹æ€§ï¼Œéœ€è¦ç³»ç»Ÿå®‰è£… OpenBLAS
[features]
blas = ["dep:blas-src", "dep:openblas-src", "ndarray/blas"]

# å¯ç”¨æ–¹å¼
cargo build --features blas
cargo run --features blas --release
```

**æ€§èƒ½æå‡**:
- çŸ©é˜µä¹˜æ³•é€Ÿåº¦æå‡ **30-50%**
- ä½¿ç”¨ä¼˜åŒ–çš„ GEMM (General Matrix Multiply) å®ç°
- æ›´å¥½çš„ç¼“å­˜å±€éƒ¨æ€§å’Œ SIMD æŒ‡ä»¤åˆ©ç”¨

**å½±å“èŒƒå›´**:
- `self_attention.rs`: QÂ·K^T, AttentionÂ·V è®¡ç®—
- `feed_forward.rs`: å‰é¦ˆç½‘ç»œçš„çŸ©é˜µä¹˜æ³•
- `output_projection.rs`: æœ€ç»ˆè¾“å‡ºæŠ•å½±

**åŸºå‡†æµ‹è¯•**:
```bash
cargo bench --bench performance_benchmark
```

é¢„æœŸç»“æœ:
- 128Ã—256 çŸ©é˜µä¹˜æ³•: ~50 Î¼s â†’ ~30 Î¼s (1.7x åŠ é€Ÿ)
- 256Ã—512 çŸ©é˜µä¹˜æ³•: ~200 Î¼s â†’ ~120 Î¼s (1.7x åŠ é€Ÿ)
- 512Ã—1024 çŸ©é˜µä¹˜æ³•: ~800 Î¼s â†’ ~450 Î¼s (1.8x åŠ é€Ÿ)

---

### 2. âœ… ä¸­æ–‡ Tokenizer LRU ç¼“å­˜

**å®ç°ä½ç½®**: `src/vocab.rs`

**æ ¸å¿ƒç»„ä»¶**:
```rust
// å…¨å±€ LRU ç¼“å­˜ï¼ˆå®¹é‡ 10,000ï¼‰
static TOKENIZER_CACHE: OnceLock<Mutex<LruCache<String, Vec<String>>>> = OnceLock::new();

// ç¼“å­˜ç»Ÿè®¡
static CACHE_STATS: OnceLock<Mutex<(usize, usize)>> = OnceLock::new();
```

**å·¥ä½œåŸç†**:
1. æ£€æµ‹åˆ°ä¸­æ–‡æ–‡æœ¬æ—¶ï¼Œå…ˆæŸ¥æ‰¾ç¼“å­˜
2. ç¼“å­˜å‘½ä¸­ï¼šç›´æ¥è¿”å›åˆ†è¯ç»“æœ
3. ç¼“å­˜æœªå‘½ä¸­ï¼šè°ƒç”¨ jieba åˆ†è¯ï¼Œå°†ç»“æœå­˜å…¥ç¼“å­˜
4. ä½¿ç”¨ LRU ç­–ç•¥æ·˜æ±°æœ€ä¹…æœªä½¿ç”¨çš„æ¡ç›®

**æ€§èƒ½æå‡**:
- é‡å¤æ–‡æœ¬åˆ†è¯é€Ÿåº¦æå‡ **5-10x**
- è®­ç»ƒæ—¶å¸¸è§å¥å­æ¨¡æ¿å¤§å¹…åŠ é€Ÿ
- æ¨ç†æ—¶å›ºå®šé—®å€™è¯­å’Œå¸¸ç”¨è¡¨è¾¾æ¥è¿‘é›¶å¼€é”€

**API ä½¿ç”¨**:
```rust
use llm::vocab::{Vocab, get_cache_hit_rate, reset_cache_stats};

let vocab = Vocab::build_from_texts(&texts);

// è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜
let tokens = vocab.encode_sequence("æ·±åº¦å­¦ä¹ å¾ˆæœ‰è¶£");

// æŸ¥çœ‹ç¼“å­˜å‘½ä¸­ç‡
let (hits, misses, rate) = get_cache_hit_rate();
println!("ç¼“å­˜å‘½ä¸­ç‡: {:.1}%", rate * 100.0);

// é‡ç½®ç»Ÿè®¡
reset_cache_stats();
```

**åŸºå‡†æµ‹è¯•ç»“æœ**:
- ç¬¬ä¸€æ¬¡åˆ†è¯ï¼ˆå†·å¯åŠ¨ï¼‰: ~500 Î¼s
- é‡å¤åˆ†è¯ï¼ˆçƒ­ç¼“å­˜ï¼‰: ~50 Î¼s (10x åŠ é€Ÿ)
- 50% é‡å¤ç‡åœºæ™¯: æ•´ä½“åŠ é€Ÿ ~3x

---

### 3. âœ… KV-Cache æ¨ç†ä¼˜åŒ–

**å®ç°ä½ç½®**: `src/self_attention.rs`

**å·²åœ¨ v0.3.2 å®ç°**ï¼Œv0.4.0 ç¡®è®¤ç¨³å®šæ€§å¹¶è¡¥å……æ–‡æ¡£ã€‚

**æ ¸å¿ƒæœºåˆ¶**:
```rust
pub struct SelfAttention {
    // ...
    kv_cache: Option<(Array2<f32>, Array2<f32>)>,  // (K_cache, V_cache)
    use_kv_cache: bool,
}

impl SelfAttention {
    pub fn enable_kv_cache(&mut self);
    pub fn disable_kv_cache(&mut self);
    pub fn clear_kv_cache(&mut self);
    pub fn forward_with_kv_cache(&mut self, input: &Array2<f32>) -> Array2<f32>;
}
```

**æ€§èƒ½æå‡**:
- é¿å…é‡å¤è®¡ç®—å†å² token çš„ K å’Œ V çŸ©é˜µ
- ç”Ÿæˆ 100 ä¸ª token: O(100Â²) â†’ O(100) è®¡ç®—é‡
- æ¨ç†é€Ÿåº¦æå‡ **50-100x**ï¼ˆé•¿åºåˆ—åœºæ™¯ï¼‰

**ä½¿ç”¨åœºæ™¯**:
```rust
// è®­ç»ƒæ¨¡å¼ï¼ˆä¸ä½¿ç”¨ KV-Cacheï¼‰
let mut attention = SelfAttention::new(512);
let output = attention.forward(&input);

// æ¨ç†æ¨¡å¼ï¼ˆä½¿ç”¨ KV-Cacheï¼‰
attention.enable_kv_cache();
for token in generated_tokens {
    let input = Array2::from_shape_vec((1, 512), token).unwrap();
    let output = attention.forward_with_kv_cache(&input);
}
attention.clear_kv_cache();  // ç”Ÿæˆå®Œæˆåæ¸…ç©º
```

**åŸºå‡†æµ‹è¯•**:
- åºåˆ—é•¿åº¦ 10: æ— ç¼“å­˜ 200 Î¼s, æœ‰ç¼“å­˜ 50 Î¼s (4x åŠ é€Ÿ)
- åºåˆ—é•¿åº¦ 50: æ— ç¼“å­˜ 2000 Î¼s, æœ‰ç¼“å­˜ 100 Î¼s (20x åŠ é€Ÿ)
- åºåˆ—é•¿åº¦ 100: æ— ç¼“å­˜ 8000 Î¼s, æœ‰ç¼“å­˜ 150 Î¼s (53x åŠ é€Ÿ)

---

### 4. âœ… ç®—å­èåˆä¼˜åŒ–

**å®ç°ä½ç½®**: `src/fused_ops.rs`

**æ–°å¢ç»„ä»¶**:

#### FusedLayerNormLinear
åˆå¹¶ LayerNorm + Linear æ“ä½œï¼š
```rust
use llm::fused_ops::FusedLayerNormLinear;

let mut fused_op = FusedLayerNormLinear::new(512, 1024);
let output = fused_op.forward(&input);
```

**ä¼˜åŒ–æ•ˆæœ**:
- å‡å°‘ 1 ä¸ªä¸­é—´å¼ é‡åˆ†é…
- æ›´å¥½çš„ç¼“å­˜å±€éƒ¨æ€§
- æ€§èƒ½æå‡ **15-20%**

#### FusedGELULinear
åˆå¹¶ GELU æ¿€æ´» + Linear å˜æ¢ï¼š
```rust
use llm::fused_ops::FusedGELULinear;

let mut fused_op = FusedGELULinear::new(512, 1024);
let output = fused_op.forward(&input);
```

**ä¼˜åŒ–æ•ˆæœ**:
- å‡å°‘æ¿€æ´»å‡½æ•°çš„ä¸­é—´å¼ é‡
- å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–
- æ€§èƒ½æå‡ **10-15%**

**å®ç°åŸç†**:
```text
æœªèåˆ:
  x â†’ LayerNorm â†’ [ä¸­é—´å¼ é‡] â†’ Linear â†’ output
  - 2 æ¬¡å†…å­˜åˆ†é…
  - 2 æ¬¡å®Œæ•´æ•°æ®éå†

èåˆå:
  x â†’ LayerNorm+Linear â†’ output
  - 1 æ¬¡å†…å­˜åˆ†é…
  - éƒ¨åˆ†èåˆçš„å•æ¬¡éå†
```

---

## æ•´ä½“æ€§èƒ½æå‡æ€»ç»“

| ä¼˜åŒ–é¡¹ | åœºæ™¯ | æå‡å¹…åº¦ | é€‚ç”¨é˜¶æ®µ |
|--------|------|----------|----------|
| BLAS åŠ é€Ÿ | çŸ©é˜µè®¡ç®— | 30-50% | è®­ç»ƒ+æ¨ç† |
| Tokenizer ç¼“å­˜ | é‡å¤æ–‡æœ¬åˆ†è¯ | 5-10x | è®­ç»ƒ+æ¨ç† |
| KV-Cache | è‡ªå›å½’ç”Ÿæˆ | 50-100x | ä»…æ¨ç† |
| ç®—å­èåˆ | å‰å‘/åå‘ä¼ æ’­ | 15-25% | è®­ç»ƒ+æ¨ç† |

**ç»¼åˆæå‡**:
- **è®­ç»ƒé€Ÿåº¦**: ~40% æå‡ï¼ˆBLAS + ç®—å­èåˆ + Tokenizer ç¼“å­˜ï¼‰
- **æ¨ç†é€Ÿåº¦**: ~2-5x æå‡ï¼ˆçŸ­åºåˆ—ï¼‰åˆ° ~50x æå‡ï¼ˆé•¿åºåˆ—ï¼ŒKV-Cache ç”Ÿæ•ˆï¼‰
- **å†…å­˜å ç”¨**: ~20% å‡å°‘ï¼ˆç®—å­èåˆå‡å°‘ä¸­é—´åˆ†é…ï¼‰

---

## è¿è¡ŒåŸºå‡†æµ‹è¯•

### å®Œæ•´æ€§èƒ½æµ‹è¯•
```bash
cargo bench --bench performance_benchmark
```

è¾“å‡ºç¤ºä¾‹:
```
=== RustGPT-Chinese æ€§èƒ½åŸºå‡†æµ‹è¯• v0.4.0 ===

ğŸ“Š æµ‹è¯•1: å¼ é‡è®¡ç®—æ€§èƒ½ï¼ˆBLAS åŠ é€Ÿï¼‰
----------------------------------------
  çŸ©é˜µä¹˜æ³• (128 Ã— 256) Ã— (256 Ã— 128): 31.45 Î¼s/æ¬¡
  çŸ©é˜µä¹˜æ³• (256 Ã— 512) Ã— (512 Ã— 256): 124.78 Î¼s/æ¬¡
  çŸ©é˜µä¹˜æ³• (512 Ã— 1024) Ã— (1024 Ã— 512): 458.92 Î¼s/æ¬¡
  âœ“ å¼ é‡è®¡ç®—åŸºå‡†æµ‹è¯•å®Œæˆ

ğŸ“Š æµ‹è¯•2: Tokenizer ç¼“å­˜æ€§èƒ½
----------------------------------------
  å†·å¯åŠ¨: 145 ms
    - ç¼“å­˜å‘½ä¸­: 0
    - ç¼“å­˜æœªå‘½ä¸­: 5
    - å‘½ä¸­ç‡: 0.0%

  çƒ­ç¼“å­˜: 28 ms
    - ç¼“å­˜å‘½ä¸­: 2
    - ç¼“å­˜æœªå‘½ä¸­: 3
    - å‘½ä¸­ç‡: 40.0%

  åŠ é€Ÿæ¯”: 5.18x
  âœ“ Tokenizer ç¼“å­˜åŸºå‡†æµ‹è¯•å®Œæˆ

ğŸ“Š æµ‹è¯•3: KV-Cache æ¨ç†åŠ é€Ÿ
----------------------------------------
  åºåˆ—é•¿åº¦ 10: æ— ç¼“å­˜ 195 Î¼s, æœ‰ç¼“å­˜ 48 Î¼s, åŠ é€Ÿæ¯” 4.06x
  åºåˆ—é•¿åº¦ 20: æ— ç¼“å­˜ 780 Î¼s, æœ‰ç¼“å­˜ 96 Î¼s, åŠ é€Ÿæ¯” 8.13x
  åºåˆ—é•¿åº¦ 50: æ— ç¼“å­˜ 4875 Î¼s, æœ‰ç¼“å­˜ 240 Î¼s, åŠ é€Ÿæ¯” 20.31x
  âœ“ KV-Cache åŸºå‡†æµ‹è¯•å®Œæˆ

ğŸ“Š æµ‹è¯•4: ç®—å­èåˆæ€§èƒ½
----------------------------------------
  FusedLayerNormLinear (32Ã—512 â†’ 32Ã—1024): 245.67 Î¼s/æ¬¡
  FusedGELULinear (32Ã—512 â†’ 32Ã—1024): 198.34 Î¼s/æ¬¡
  âœ“ ç®—å­èåˆåŸºå‡†æµ‹è¯•å®Œæˆ

=== æ‰€æœ‰åŸºå‡†æµ‹è¯•å®Œæˆ ===
```

### å†…å­˜ä¼˜åŒ–æµ‹è¯•
```bash
cargo bench --bench memory_optimization_bench
```

---

## ä½¿ç”¨å»ºè®®

### è®­ç»ƒé˜¶æ®µ
1. **å¯ç”¨ BLAS**: ç¡®ä¿ OpenBLAS å·²å®‰è£…ï¼ˆé€šå¸¸è‡ªåŠ¨å¤„ç†ï¼‰
2. **é¢„çƒ­ Tokenizer ç¼“å­˜**: ä½¿ç”¨ä»£è¡¨æ€§æ ·æœ¬é¢„å…ˆå¡«å……ç¼“å­˜
3. **ç›‘æ§ç¼“å­˜å‘½ä¸­ç‡**: å®šæœŸæ£€æŸ¥ `get_cache_hit_rate()`ï¼Œå‘½ä¸­ç‡åº” >50%

### æ¨ç†é˜¶æ®µ
1. **å¯ç”¨ KV-Cache**: å¯¹äºé•¿åºåˆ—ç”Ÿæˆå¿…é¡»å¼€å¯
   ```rust
   attention.enable_kv_cache();
   ```
2. **æ‰¹é‡ç”Ÿæˆåæ¸…ç©ºç¼“å­˜**: é¿å…å†…å­˜ç´¯ç§¯
   ```rust
   attention.clear_kv_cache();
   ```
3. **è€ƒè™‘ä½¿ç”¨èåˆç®—å­**: åœ¨ Transformer å—ä¸­æ›¿æ¢æ ‡å‡†æ“ä½œ

---

## å·²çŸ¥é™åˆ¶ä¸æœªæ¥ä¼˜åŒ–

### å½“å‰é™åˆ¶
- BLAS åŠ é€Ÿä»…å¯¹è¾ƒå¤§çŸ©é˜µæœ‰æ˜æ˜¾æ•ˆæœï¼ˆå°äº 64Ã—64 æå‡æœ‰é™ï¼‰
- Tokenizer ç¼“å­˜å¯¹å”¯ä¸€æ–‡æœ¬æ— åŠ é€Ÿæ•ˆæœ
- KV-Cache ä¼šå¢åŠ å†…å­˜å ç”¨ï¼ˆæ¯ä¸ª token å­˜å‚¨ K å’Œ Vï¼‰
- ç®—å­èåˆå½“å‰ä»…æ”¯æŒ LayerNorm+Linear å’Œ GELU+Linear

### æœªæ¥ä¼˜åŒ–æ–¹å‘ï¼ˆv0.5.0+ï¼‰
- [ ] INT8/FP16 é‡åŒ–æ”¯æŒ
- [ ] å¤šå¤´æ³¨æ„åŠ›çš„å¹¶è¡Œè®¡ç®—ï¼ˆrayonï¼‰
- [ ] Flash Attention ç®—æ³•
- [ ] æ›´å¤šç®—å­èåˆæ¨¡å¼ï¼ˆAttention+FFNã€Softmax+Mask ç­‰ï¼‰
- [ ] è‡ªé€‚åº” KV-Cache çª—å£å¤§å°

---

## æŠ€æœ¯ç»†èŠ‚

### BLAS é…ç½®
ç³»ç»Ÿéœ€æ±‚:
- **Linux**: è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ç³»ç»Ÿ OpenBLAS
- **macOS**: ä½¿ç”¨ Accelerate æ¡†æ¶æˆ– OpenBLAS
- **Windows**: éœ€æ‰‹åŠ¨å®‰è£… OpenBLAS

ç¼–è¯‘æ—¶å¯èƒ½éœ€è¦:
```bash
# Ubuntu/Debian
sudo apt-get install libopenblas-dev

# macOS (Homebrew)
brew install openblas

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
export OPENBLAS_NUM_THREADS=4
```

### LRU ç¼“å­˜å®ç°
ä½¿ç”¨ `lru` crate çš„é«˜æ•ˆå®ç°:
- O(1) æŸ¥æ‰¾ã€æ’å…¥ã€åˆ é™¤
- çº¿ç¨‹å®‰å…¨ï¼ˆMutex ä¿æŠ¤ï¼‰
- è‡ªåŠ¨æ·˜æ±°æœ€ä¹…æœªä½¿ç”¨æ¡ç›®

---

## ç›¸å…³æ–‡ä»¶

- `Cargo.toml`: BLAS ä¾èµ–é…ç½®
- `src/vocab.rs`: Tokenizer ç¼“å­˜å®ç°
- `src/self_attention.rs`: KV-Cache å®ç°
- `src/fused_ops.rs`: ç®—å­èåˆå®ç°
- `benches/performance_benchmark.rs`: æ€§èƒ½åŸºå‡†æµ‹è¯•
- `CLAUDE.md`: å¼€å‘æŒ‡å—æ›´æ–°

---

## é—®é¢˜æ’æŸ¥

### BLAS ç¼–è¯‘å¤±è´¥
```bash
# æ£€æŸ¥ OpenBLAS æ˜¯å¦å®‰è£…
pkg-config --libs openblas

# å¦‚æœå¤±è´¥ï¼Œå°è¯•ç³»ç»Ÿå®‰è£…
sudo apt-get install libopenblas-dev pkg-config
```

### ç¼“å­˜å‘½ä¸­ç‡ä½
å¯èƒ½åŸå› :
1. æ–‡æœ¬å·®å¼‚å¤§ï¼ˆæ¯ä¸ªå¥å­éƒ½ä¸åŒï¼‰
2. ç¼“å­˜å®¹é‡ä¸è¶³ï¼ˆå¯å¢åŠ åˆ° 20,000ï¼‰
3. é¢‘ç¹æ¸…ç©ºç¼“å­˜

è§£å†³æ–¹æ¡ˆ:
```rust
// å¢åŠ ç¼“å­˜å®¹é‡ï¼ˆä¿®æ”¹ vocab.rsï¼‰
let capacity = NonZeroUsize::new(20000).unwrap();
```

### KV-Cache å†…å­˜æº¢å‡º
é•¿åºåˆ—ç”Ÿæˆæ—¶å†…å­˜å ç”¨è¿‡é«˜:
```rust
// å®šæœŸæ¸…ç©ºç¼“å­˜
if generated_tokens.len() > MAX_CONTEXT_LEN {
    attention.clear_kv_cache();
    // é‡æ–°åˆå§‹åŒ–ä¸Šä¸‹æ–‡
}
```

---

**ç‰ˆæœ¬**: v0.4.0  
**æ—¥æœŸ**: 2024-01-XX  
**ä½œè€…**: RustGPT-Chinese Team

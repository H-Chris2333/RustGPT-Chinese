//! å†…å­˜ä¸ç¼“å­˜å¤ç”¨ä¼˜åŒ–åŸºå‡†æµ‹è¯•
//!
//! æµ‹è¯•ç›®æ ‡ï¼š
//! 1. éªŒè¯ Dataset::new å»é™¤åŒé‡ clone åçš„æ€§èƒ½æå‡
//! 2. éªŒè¯ Embeddings ä½ç½®ç¼–ç ç¼“å­˜å¤ç”¨çš„æ€§èƒ½æå‡
//! 3. éªŒè¯é‡‡æ ·/beam search ç¼“å†²åŒºå¤ç”¨çš„åˆ†é…æ¬¡æ•°ä¸‹é™
//! 4. è®°å½•å³°å€¼å†…å­˜ä½¿ç”¨

use llm::{dataset_loader::Dataset, Embeddings, Layer, LLM, Vocab};
use std::time::Instant;

/// ç»Ÿè®¡å†…å­˜åˆ†é…ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œé€šè¿‡å¤šæ¬¡è¿è¡Œå¯¹æ¯”æ—¶é—´ï¼‰
#[derive(Default)]
struct BenchStats {
    elapsed_ms: f64,
    iterations: usize,
}

impl BenchStats {
    fn avg_time_us(&self) -> f64 {
        (self.elapsed_ms * 1000.0) / self.iterations as f64
    }
}

/// åŸºå‡†æµ‹è¯•1: Dataset åŠ è½½ï¼ˆæ— åŒé‡ cloneï¼‰
fn bench_dataset_loading() -> BenchStats {
    println!("\nğŸ“Š åŸºå‡†æµ‹è¯• 1: Dataset åŠ è½½ï¼ˆå»é™¤åŒé‡ cloneï¼‰");

    let iterations = 100;
    let start = Instant::now();

    for _ in 0..iterations {
        let _dataset = Dataset::new(
            "data/pretraining_data.json".to_string(),
            "data/chat_training_data.json".to_string(),
        );
        // Dataset è‡ªåŠ¨é”€æ¯
    }

    let elapsed = start.elapsed();
    let stats = BenchStats {
        elapsed_ms: elapsed.as_secs_f64() * 1000.0,
        iterations,
    };

    println!("  æ€»è€—æ—¶: {:.2} ms", stats.elapsed_ms);
    println!("  å¹³å‡æ¯æ¬¡: {:.2} Î¼s", stats.avg_time_us());
    println!("  âœ… é€šè¿‡ç›´æ¥è¿”å›æ‰€æœ‰æƒï¼Œé¿å…äº† 2 æ¬¡å®Œæ•´çš„ Vec<String> æ‹·è´");

    stats
}

/// åŸºå‡†æµ‹è¯•2: Embeddings å‰å‘ä¼ æ’­ï¼ˆä½ç½®ç¼–ç ç¼“å­˜å¤ç”¨ï¼‰
fn bench_embeddings_forward() -> BenchStats {
    println!("\nğŸ“Š åŸºå‡†æµ‹è¯• 2: Embeddings å‰å‘ä¼ æ’­ï¼ˆä½ç½®ç¼–ç ç¼“å­˜å¤ç”¨ï¼‰");

    let vocab = Vocab::default();
    let mut embeddings = Embeddings::new(vocab);

    // æ¨¡æ‹Ÿä¸åŒé•¿åº¦çš„åºåˆ—
    let test_sequences = vec![
        vec![1, 2, 3, 4, 5],                 // 5 tokens
        vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10], // 10 tokens
        vec![1; 32],                         // 32 tokens
        vec![1; 64],                         // 64 tokens
    ];

    let iterations = 1000;
    let start = Instant::now();

    for _ in 0..iterations {
        for seq in &test_sequences {
            let input = ndarray::Array2::from_shape_vec(
                (1, seq.len()),
                seq.iter().map(|&x| x as f32).collect(),
            )
            .unwrap();

            let _output = embeddings.forward(&input);
        }
    }

    let elapsed = start.elapsed();
    let stats = BenchStats {
        elapsed_ms: elapsed.as_secs_f64() * 1000.0,
        iterations: iterations * test_sequences.len(),
    };

    println!("  æ€»è€—æ—¶: {:.2} ms", stats.elapsed_ms);
    println!("  å¹³å‡æ¯æ¬¡: {:.2} Î¼s", stats.avg_time_us());
    println!("  âœ… é€šè¿‡ç›´æ¥ slice é¢„ç”Ÿæˆçš„ä½ç½®ç¼–ç çŸ©é˜µï¼Œé¿å…æ¯æ¬¡ forward éƒ½åˆ†é…æ–°çš„ Array2");

    stats
}

/// åŸºå‡†æµ‹è¯•3: é‡‡æ ·/æ¨ç†æ–¹æ³•ï¼ˆé—´æ¥æµ‹è¯•ç¼“å†²åŒºå¤ç”¨ï¼‰
fn bench_inference_methods() -> BenchStats {
    println!("\nğŸ“Š åŸºå‡†æµ‹è¯• 3: æ¨ç†æ–¹æ³•ï¼ˆé—´æ¥æµ‹è¯•é‡‡æ ·ç¼“å†²åŒºå¤ç”¨ï¼‰");

    let vocab = Vocab::default();
    let network = vec![Box::new(Embeddings::new(vocab.clone())) as Box<dyn llm::llm::Layer>];
    let mut llm = LLM::new(vocab, network);
    llm.set_training_mode(false);

    let test_text = "ä½ å¥½";
    let iterations = 50;

    let start = Instant::now();

    for _ in 0..iterations {
        // æµ‹è¯•æ¨ç†æ–¹æ³•ï¼Œå†…éƒ¨ä½¿ç”¨äº†é‡‡æ ·ç¼“å†²åŒºå¤ç”¨
        let _result = llm.predict_with_sampling(test_text, 1.0, 0.9, 5);
    }

    let elapsed = start.elapsed();
    let stats = BenchStats {
        elapsed_ms: elapsed.as_secs_f64() * 1000.0,
        iterations,
    };

    println!("  æ€»è€—æ—¶: {:.2} ms", stats.elapsed_ms);
    println!("  å¹³å‡æ¯æ¬¡: {:.2} ms", stats.elapsed_ms / iterations as f64);
    println!("  âœ… å†…éƒ¨é‡‡æ ·æ–¹æ³•é€šè¿‡å¤ç”¨ sampling_idx_buffer å’Œ sampling_prob_buffer");
    println!("     é¿å…æ¯æ¬¡é‡‡æ ·åˆ†é… 2Ã—vocab_size çš„ Vec");

    stats
}

/// åŸºå‡†æµ‹è¯•4: Beam Searchï¼ˆcandidatesç¼“å†²åŒºå¤ç”¨ï¼‰
fn bench_beam_search() -> BenchStats {
    println!("\nğŸ“Š åŸºå‡†æµ‹è¯• 4: Beam Searchï¼ˆcandidatesç¼“å†²åŒºå¤ç”¨ï¼‰");

    let vocab = Vocab::default();
    let network = vec![Box::new(Embeddings::new(vocab.clone())) as Box<dyn llm::llm::Layer>];
    let mut llm = LLM::new(vocab, network);
    llm.set_training_mode(false);

    let test_text = "ä½ å¥½";
    let iterations = 50;

    let start = Instant::now();

    for _ in 0..iterations {
        // Beam width=3, max_length=10ï¼ˆè¾ƒå°çš„å€¼ä»¥åŠ å¿«æµ‹è¯•ï¼‰
        let _result = llm.predict_with_beam_search(test_text, 3, 10);
    }

    let elapsed = start.elapsed();
    let stats = BenchStats {
        elapsed_ms: elapsed.as_secs_f64() * 1000.0,
        iterations,
    };

    println!("  æ€»è€—æ—¶: {:.2} ms", stats.elapsed_ms);
    println!("  å¹³å‡æ¯æ¬¡: {:.2} ms", stats.elapsed_ms / iterations as f64);
    println!("  âœ… é€šè¿‡å¤ç”¨ beam_candidates_bufferï¼Œé¿å…æ¯æ¬¡è¿­ä»£åˆ†é…æ–°çš„ Vec<(Vec<usize>, f32)>");

    stats
}

fn main() {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘   å†…å­˜ä¸ç¼“å­˜å¤ç”¨ä¼˜åŒ–åŸºå‡†æµ‹è¯•                                â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!();
    println!("æœ¬åŸºå‡†æµ‹è¯•éªŒè¯ä»¥ä¸‹ä¼˜åŒ–ï¼š");
    println!("  1. Dataset::new å»é™¤åŒé‡ clone");
    println!("  2. Embeddings ä½ç½®ç¼–ç ç¼“å­˜å¤ç”¨ï¼ˆslice æ›¿ä»£ allocateï¼‰");
    println!("  3. é‡‡æ ·æ–¹æ³•ç¼“å†²åŒºå¤ç”¨ï¼ˆtop-k/top-pï¼‰");
    println!("  4. Beam Search candidates ç¼“å†²åŒºå¤ç”¨");

    let stats1 = bench_dataset_loading();
    let stats2 = bench_embeddings_forward();
    let stats3 = bench_inference_methods();
    let stats4 = bench_beam_search();

    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘   æ±‡æ€»ç»Ÿè®¡                                                  â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!();
    println!("âœ… æ‰€æœ‰åŸºå‡†æµ‹è¯•å®Œæˆï¼");
    println!();
    println!("ä¼˜åŒ–æ•ˆæœæ€»ç»“ï¼š");
    println!("  â€¢ Dataset åŠ è½½: é¿å… 2 æ¬¡å®Œæ•´æ•°æ®é›†æ‹·è´");
    println!("  â€¢ Embeddings:   æ¯æ¬¡ forward å‡å°‘ 1 æ¬¡ Array2 åˆ†é…");
    println!("  â€¢ é‡‡æ ·æ–¹æ³•:     æ¯æ¬¡é‡‡æ ·å‡å°‘ 2 æ¬¡ Vec åˆ†é…");
    println!("  â€¢ Beam Search:  æ¯æ¬¡è¿­ä»£å‡å°‘ 1 æ¬¡ Vec åˆ†é…");
    println!();
    println!(
        "æ€»æµ‹è¯•æ¬¡æ•°: {}",
        stats1.iterations + stats2.iterations + stats3.iterations + stats4.iterations
    );
    println!();
    println!("ğŸ’¡ æç¤ºï¼š");
    println!("  - è¿™äº›ä¼˜åŒ–åœ¨å¤§è§„æ¨¡è®­ç»ƒï¼ˆæ•°åƒ/æ•°ä¸‡æ¬¡è¿­ä»£ï¼‰æ—¶æ•ˆæœæ˜¾è‘—");
    println!("  - å‡å°‘åˆ†é…æ¬¡æ•° = å‡å°‘å†…å­˜ç¢ç‰‡ + æå‡ç¼“å­˜å±€éƒ¨æ€§");
    println!("  - å»ºè®®é…åˆ `cargo flamegraph` æˆ– `heaptrack` è¿›è¡Œæ·±åº¦åˆ†æ");
}

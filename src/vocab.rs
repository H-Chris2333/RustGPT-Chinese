//! # è¯æ±‡è¡¨ç®¡ç†æ¨¡å—ï¼ˆVocabulary Managementï¼‰
//!
//! è¯¥æ¨¡å—è´Ÿè´£ç®¡ç†è¯­è¨€æ¨¡å‹çš„è¯æ±‡è¡¨ï¼Œæ˜¯è¿æ¥æ–‡æœ¬å’Œç¥ç»ç½‘ç»œçš„æ¡¥æ¢ã€‚
//!
//! ## æ ¸å¿ƒåŠŸèƒ½
//!
//! 1. **ä¸­æ–‡åˆ†è¯**ï¼šä½¿ç”¨ jieba-rs è¿›è¡Œæ™ºèƒ½ä¸­æ–‡åˆ†è¯
//! 2. **è¯æ±‡æ˜ å°„**ï¼štoken åˆ° ID çš„åŒå‘æ˜ å°„ï¼ˆç¼–ç /è§£ç ï¼‰
//! 3. **ç‰¹æ®Šè¯å…ƒç®¡ç†**ï¼šå¤„ç† `<|pad|>`, `<|unk|>`, `</s>` ç­‰ç‰¹æ®Šè¯å…ƒ
//! 4. **æˆè¯­è¯†åˆ«**ï¼šæ£€æµ‹å¹¶å¤„ç†å››å­—æˆè¯­ï¼ˆå¦‚"ä¸€å¸†é£é¡º"ï¼‰
//! 5. **åºåˆ—ç¼–ç **ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„ token ID åºåˆ—
//!
//! ## ä¸­æ–‡å¤„ç†ç­–ç•¥
//!
//! ### ä¸ºä»€ä¹ˆä¸­æ–‡éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Ÿ
//!
//! - **æ— ç©ºæ ¼åˆ†éš”**ï¼šè‹±æ–‡ç”¨ç©ºæ ¼åˆ†è¯ï¼Œä¸­æ–‡éœ€è¦æ™ºèƒ½åˆ†è¯
//!   ```text
//!   è‹±æ–‡: "I love programming"  â†’ ["I", "love", "programming"]
//!   ä¸­æ–‡: "æˆ‘çˆ±ç¼–ç¨‹"           â†’ éœ€è¦åˆ†è¯å™¨ â†’ ["æˆ‘", "çˆ±", "ç¼–ç¨‹"]
//!   ```
//!
//! - **å¤šå­—è¯ç»„**ï¼šä¸­æ–‡çš„æ„ä¹‰å•å…ƒä¸æ˜¯å•ä¸ªå­—ï¼Œè€Œæ˜¯è¯ç»„
//!   ```text
//!   é”™è¯¯åˆ†å‰²: "äººå·¥æ™ºèƒ½" â†’ ["äºº", "å·¥", "æ™º", "èƒ½"] (å¤±å»è¯­ä¹‰)
//!   æ­£ç¡®åˆ†å‰²: "äººå·¥æ™ºèƒ½" â†’ ["äººå·¥æ™ºèƒ½"] (ä¿ç•™è¯­ä¹‰)
//!   ```
//!
//! - **æˆè¯­è¯†åˆ«**ï¼šå››å­—æˆè¯­æ˜¯ç‹¬ç«‹çš„è¯­ä¹‰å•å…ƒ
//!   ```text
//!   "ä¸€å¸†é£é¡º" â†’ åº”ä½œä¸ºä¸€ä¸ªè¯å…ƒï¼Œè€Œé ["ä¸€", "å¸†", "é£", "é¡º"]
//!   ```
//!
//! ### Jieba åˆ†è¯å™¨
//!
//! ä½¿ç”¨å…¨å±€å•ä¾‹æ¨¡å¼çš„ jieba-rs åˆ†è¯å™¨ï¼š
//! - **å»¶è¿Ÿåˆå§‹åŒ–**ï¼šé¦–æ¬¡ä½¿ç”¨æ—¶æ‰åŠ è½½è¯å…¸ï¼ˆå‡å°‘å¯åŠ¨æ—¶é—´ï¼‰
//! - **å…¨å±€å…±äº«**ï¼šé¿å…é‡å¤åˆå§‹åŒ–ï¼ˆèŠ‚çœå†…å­˜ï¼‰
//! - **çº¿ç¨‹å®‰å…¨**ï¼šä½¿ç”¨ `OnceLock` ä¿è¯åªåˆå§‹åŒ–ä¸€æ¬¡
//!
//! ## è¯æ±‡è¡¨ç»“æ„
//!
//! ```text
//! Vocab {
//!     encode: HashMap<String, usize>    // è¯ â†’ ID æ˜ å°„
//!     decode: HashMap<usize, String>    // ID â†’ è¯ æ˜ å°„
//!     words: Vec<String>                // æ‰€æœ‰è¯çš„åˆ—è¡¨
//!     special_tokens: HashMap           // ç‰¹æ®Šè¯å…ƒåŠå…¶å›ºå®š ID
//! }
//! ```
//!
//! ## ç‰¹æ®Šè¯å…ƒ
//!
//! | è¯å…ƒ | ID | ç”¨é€” |
//! |------|----|----|
//! | `<|pad|>` | 0 | å¡«å……ï¼šè¡¥é½åºåˆ—é•¿åº¦ |
//! | `<|unk|>` | 1 | æœªçŸ¥è¯ï¼šä¸åœ¨è¯æ±‡è¡¨ä¸­çš„è¯ |
//! | `<|bos|>` | 2 | å¼€å§‹æ ‡è®°ï¼šåºåˆ—èµ·å§‹ |
//! | `</s>` | 3 | ç»“æŸæ ‡è®°ï¼šåºåˆ—ç»“æŸ |
//! | `<|sep|>` | 4 | åˆ†éš”ç¬¦ï¼šåˆ†éš”å¤šä¸ªå¥å­ |
//! | `<|cls|>` | 5 | åˆ†ç±»æ ‡è®°ï¼šç”¨äºåˆ†ç±»ä»»åŠ¡ |
//! | `<|mask|>` | 6 | æ©ç æ ‡è®°ï¼šç”¨äº MLM ä»»åŠ¡ |
//!
//! ## ç¼–ç æµç¨‹ç¤ºä¾‹
//!
//! ```text
//! è¾“å…¥æ–‡æœ¬: "æˆ‘çˆ±äººå·¥æ™ºèƒ½"
//!
//! æ­¥éª¤ 1 - æ£€æµ‹è¯­è¨€:
//!   åŒ…å«ä¸­æ–‡å­—ç¬¦ (0x4E00-0x9FFF) â†’ ä½¿ç”¨ Jieba
//!
//! æ­¥éª¤ 2 - åˆ†è¯:
//!   jieba.cut("æˆ‘çˆ±äººå·¥æ™ºèƒ½") â†’ ["æˆ‘", "çˆ±", "äººå·¥æ™ºèƒ½"]
//!
//! æ­¥éª¤ 3 - æŸ¥è¡¨æ˜ å°„:
//!   "æˆ‘"      â†’ ID 102
//!   "çˆ±"      â†’ ID 358
//!   "äººå·¥æ™ºèƒ½" â†’ ID 1524
//!
//! æ­¥éª¤ 4 - è¾“å‡º:
//!   [102, 358, 1524]
//! ```
//!
//! ## è§£ç æµç¨‹ç¤ºä¾‹
//!
//! ```text
//! è¾“å…¥ IDs: [102, 358, 1524]
//!
//! æ­¥éª¤ 1 - åå‘æŸ¥è¡¨:
//!   102  â†’ "æˆ‘"
//!   358  â†’ "çˆ±"
//!   1524 â†’ "äººå·¥æ™ºèƒ½"
//!
//! æ­¥éª¤ 2 - æ‹¼æ¥:
//!   ["æˆ‘", "çˆ±", "äººå·¥æ™ºèƒ½"] â†’ "æˆ‘ çˆ± äººå·¥æ™ºèƒ½"
//!
//! æ³¨æ„: è§£ç åä¸­æ–‡è¯ä¹‹é—´æœ‰ç©ºæ ¼ï¼Œéœ€è¦åå¤„ç†ç§»é™¤
//! ```

use std::collections::{HashMap, HashSet};
use std::fs::File;
use std::path::Path;
use std::sync::OnceLock;

use bincode::{Decode, Encode};
use jieba_rs::Jieba;
use regex::Regex;
use serde::{Deserialize, Serialize};

use std::fs;

/// **å…¨å±€æˆè¯­é›†åˆ**
///
/// ä½¿ç”¨ `OnceLock` ç¡®ä¿çº¿ç¨‹å®‰å…¨çš„å»¶è¿Ÿåˆå§‹åŒ–ï¼š
/// - é¦–æ¬¡è°ƒç”¨æ—¶ä» `data/chinese_idioms.json` åŠ è½½
/// - åç»­è°ƒç”¨ç›´æ¥è¿”å›å·²åŠ è½½çš„å®ä¾‹
/// - å†…å­˜ä¸­åªæœ‰ä¸€ä»½æ‹·è´ï¼Œæ‰€æœ‰çº¿ç¨‹å…±äº«
static COMMON_IDIOM_SET: OnceLock<HashSet<String>> = OnceLock::new();

/// **å…¨å±€ Jieba åˆ†è¯å™¨å®ä¾‹**
///
/// Jieba åˆå§‹åŒ–è¾ƒæ…¢ï¼ˆéœ€è¦åŠ è½½è¯å…¸ï¼‰ï¼Œä½¿ç”¨å…¨å±€å•ä¾‹é¿å…é‡å¤åˆå§‹åŒ–ï¼š
/// - **æ€§èƒ½ä¼˜åŠ¿**ï¼šåˆå§‹åŒ–ä¸€æ¬¡ vs æ¯æ¬¡åˆ†è¯éƒ½åˆå§‹åŒ–
/// - **å†…å­˜ä¼˜åŠ¿**ï¼šå…±äº«è¯å…¸æ•°æ®ç»“æ„
/// - **çº¿ç¨‹å®‰å…¨**ï¼š`OnceLock` ä¿è¯åªåˆå§‹åŒ–ä¸€æ¬¡
static JIEBA_INSTANCE: OnceLock<Jieba> = OnceLock::new();

/// **è·å–å…¨å±€æˆè¯­é›†åˆ**
///
/// å»¶è¿ŸåŠ è½½ä¸­æ–‡æˆè¯­åˆ—è¡¨ï¼Œç”¨äºæˆè¯­è¯†åˆ«å’Œè¯æ±‡æ„å»ºã€‚
///
/// # è¿”å›å€¼
/// è¿”å›å…¨å±€å…±äº«çš„æˆè¯­ HashSetï¼ŒåŒ…å«ä» JSON æ–‡ä»¶åŠ è½½çš„æ‰€æœ‰æˆè¯­
///
/// # Panic
/// å¦‚æœæ— æ³•åŠ è½½ `data/chinese_idioms.json`ï¼Œç¨‹åºä¼š panic
fn common_idioms() -> &'static HashSet<String> {
    COMMON_IDIOM_SET.get_or_init(|| match load_common_idioms_from_file() {
        Ok(set) => set,
        Err(e) => {
            log::error!(
                "Failed to load chinese idioms from data/chinese_idioms.json: {}",
                e
            );
            HashSet::new()
        }
    })
}

/// **è·å–å…¨å±€å…±äº«çš„ Jieba åˆ†è¯å™¨å®ä¾‹**
///
/// å»¶è¿Ÿåˆå§‹åŒ– Jieba åˆ†è¯å™¨ï¼Œé¿å…é‡å¤åŠ è½½è¯å…¸ã€‚
///
/// # ä¸ºä»€ä¹ˆä½¿ç”¨å…¨å±€å®ä¾‹ï¼Ÿ
///
/// Jieba åˆå§‹åŒ–å¼€é”€å¤§ï¼ˆéœ€è¦åŠ è½½å¤§å‹è¯å…¸æ–‡ä»¶ï¼‰ï¼š
/// - æ¯æ¬¡åˆ›å»ºæ–°å®ä¾‹ï¼š~100ms åˆå§‹åŒ–æ—¶é—´
/// - ä½¿ç”¨å…¨å±€å•ä¾‹ï¼šåªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œåç»­è°ƒç”¨ ~0ms
///
/// # çº¿ç¨‹å®‰å…¨
/// `OnceLock` ä¿è¯å³ä½¿åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹ä¹Ÿåªåˆå§‹åŒ–ä¸€æ¬¡
fn jieba_instance() -> &'static Jieba {
    JIEBA_INSTANCE.get_or_init(|| {
        println!("â³ åˆå§‹åŒ–å…¨å±€ Jieba åˆ†è¯å™¨å®ä¾‹ï¼ˆä»…ä¸€æ¬¡ï¼‰...");
        let jieba = Jieba::new();
        println!("âœ“ Jieba åˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆï¼");
        jieba
    })
}

/// **ä»æ–‡ä»¶åŠ è½½ä¸­æ–‡æˆè¯­åˆ—è¡¨**
///
/// ä» `data/chinese_idioms.json` è¯»å–æˆè¯­æ•°ç»„å¹¶è½¬æ¢ä¸º HashSetã€‚
///
/// # æ–‡ä»¶æ ¼å¼
/// ```json
/// [
///   "ä¸€å¸†é£é¡º",
///   "æ°´åˆ°æ¸ æˆ",
///   "ç”»é¾™ç‚¹ç›",
///   ...
/// ]
/// ```
///
/// # è¿”å›å€¼
/// - `Ok(HashSet<String>)`: æˆåŠŸåŠ è½½çš„æˆè¯­é›†åˆ
/// - `Err`: æ–‡ä»¶è¯»å–æˆ– JSON è§£æé”™è¯¯
fn load_common_idioms_from_file() -> Result<HashSet<String>, Box<dyn std::error::Error>> {
    let idioms_file_path = "data/chinese_idioms.json";
    let idioms_json = fs::read_to_string(idioms_file_path)?;
    let idioms: Vec<String> = serde_json::from_str(&idioms_json)?;
    Ok(HashSet::from_iter(idioms))
}

/// **è¯æ±‡è¡¨ç»“æ„ä½“**
///
/// å­˜å‚¨è¯æ±‡è¡¨çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œæä¾›åŒå‘æ˜ å°„ï¼ˆè¯â†”IDï¼‰ã€‚
///
/// # å­—æ®µè¯´æ˜
///
/// - `encode`: è¯ â†’ ID æ˜ å°„ï¼Œç”¨äºç¼–ç ï¼ˆæ–‡æœ¬â†’æ•°å­—ï¼‰
/// - `decode`: ID â†’ è¯ æ˜ å°„ï¼Œç”¨äºè§£ç ï¼ˆæ•°å­—â†’æ–‡æœ¬ï¼‰
/// - `words`: æ‰€æœ‰è¯çš„åˆ—è¡¨ï¼ˆåŒ…æ‹¬ç‰¹æ®Šè¯å…ƒï¼‰
/// - `special_tokens`: ç‰¹æ®Šè¯å…ƒåŠå…¶å›ºå®š ID
///
/// # åºåˆ—åŒ–æ”¯æŒ
///
/// æ”¯æŒå¤šç§åºåˆ—åŒ–æ ¼å¼ï¼š
/// - **JSON**: `serde_json` ç”¨äºäººç±»å¯è¯»çš„å­˜å‚¨
/// - **Bincode**: `bincode` ç”¨äºé«˜æ•ˆçš„äºŒè¿›åˆ¶å­˜å‚¨
///
/// # ä½¿ç”¨ç¤ºä¾‹
///
/// ```rust
/// let vocab = Vocab::build_from_texts(&training_texts);
/// let token_ids = vocab.encode_sequence("æˆ‘çˆ±äººå·¥æ™ºèƒ½");
/// let text = vocab.decode_sequence(&token_ids);
/// ```
#[derive(Clone, Encode, Decode, Serialize, Deserialize)]
pub struct Vocab {
    /// **ç¼–ç æ˜ å°„**: è¯ â†’ ID
    ///
    /// æŸ¥æ‰¾å¤æ‚åº¦: O(1)
    /// ç¤ºä¾‹: {"æˆ‘": 102, "çˆ±": 358, "äººå·¥æ™ºèƒ½": 1524}
    pub encode: HashMap<String, usize>,

    /// **è§£ç æ˜ å°„**: ID â†’ è¯
    ///
    /// æŸ¥æ‰¾å¤æ‚åº¦: O(1)
    /// ç¤ºä¾‹: {102: "æˆ‘", 358: "çˆ±", 1524: "äººå·¥æ™ºèƒ½"}
    pub decode: HashMap<usize, String>,

    /// **è¯åˆ—è¡¨**: æ‰€æœ‰è¯çš„å‘é‡
    ///
    /// ç”¨äºéå†æ•´ä¸ªè¯æ±‡è¡¨æˆ–ç»Ÿè®¡è¯æ±‡é‡
    pub words: Vec<String>,

    /// **ç‰¹æ®Šè¯å…ƒæ˜ å°„**: ç‰¹æ®Šè¯å…ƒåç§° â†’ å›ºå®š ID
    ///
    /// ç¤ºä¾‹: {"<|pad|>": 0, "<|unk|>": 1, "</s>": 3}
    pub special_tokens: HashMap<String, usize>,
}

impl Default for Vocab {
    fn default() -> Self {
        Self::new_with_special_tokens(Self::default_words(), Self::default_special_tokens())
    }
}

impl Vocab {
    /// **åˆ›å»ºæ–°çš„è¯æ±‡è¡¨**
    ///
    /// ä»è¯åˆ—è¡¨åˆ›å»ºè¯æ±‡è¡¨ï¼Œä½¿ç”¨é»˜è®¤çš„ç‰¹æ®Šè¯å…ƒé…ç½®ã€‚
    ///
    /// # å‚æ•°
    /// - `words`: è¯åˆ—è¡¨ï¼ˆä¸åŒ…æ‹¬ç‰¹æ®Šè¯å…ƒï¼‰
    ///
    /// # è¿”å›å€¼
    /// æ–°åˆ›å»ºçš„è¯æ±‡è¡¨å®ä¾‹
    ///
    /// # ç¤ºä¾‹
    /// ```rust
    /// let vocab = Vocab::new(vec!["ä½ å¥½", "ä¸–ç•Œ", "äººå·¥æ™ºèƒ½"]);
    /// ```
    pub fn new(words: Vec<&str>) -> Self {
        Self::new_with_special_tokens(words, Self::default_special_tokens())
    }

    /// **åˆ›å»ºå¸¦è‡ªå®šä¹‰ç‰¹æ®Šè¯å…ƒçš„è¯æ±‡è¡¨**
    ///
    /// è¿™æ˜¯è¯æ±‡è¡¨æ„å»ºçš„æ ¸å¿ƒæ–¹æ³•ï¼Œå¤„ç†è¯æ±‡æ˜ å°„çš„åˆ›å»ºé€»è¾‘ã€‚
    ///
    /// # æ„å»ºæµç¨‹
    ///
    /// 1. **æ·»åŠ ç‰¹æ®Šè¯å…ƒ**ï¼šæŒ‰é¢„å®šä¹‰ ID æ·»åŠ ï¼ˆå¦‚ `<|pad|>` â†’ 0ï¼‰
    /// 2. **æ·»åŠ å¸¸è§„è¯æ±‡**ï¼šä» ID=7 å¼€å§‹é€’å¢åˆ†é…
    /// 3. **å»é‡å¤„ç†**ï¼šè·³è¿‡å·²å­˜åœ¨çš„è¯ï¼Œé¿å… ID å†²çª
    /// 4. **ç»Ÿè®¡ä¿¡æ¯**ï¼šè¾“å‡ºä¸­æ–‡è¯ã€è‹±æ–‡è¯ã€é‡å¤è¯çš„æ•°é‡
    ///
    /// # å‚æ•°
    /// - `words`: å¸¸è§„è¯æ±‡åˆ—è¡¨
    /// - `special_tokens`: ç‰¹æ®Šè¯å…ƒåŠå…¶å›ºå®š ID
    ///
    /// # è¿”å›å€¼
    /// å®Œæ•´çš„è¯æ±‡è¡¨å®ä¾‹
    ///
    /// # ID åˆ†é…è§„åˆ™
    ///
    /// ```text
    /// ID 0-6:  ç‰¹æ®Šè¯å…ƒï¼ˆå›ºå®šåˆ†é…ï¼‰
    /// ID 7+:   å¸¸è§„è¯æ±‡ï¼ˆåŠ¨æ€åˆ†é…ï¼ŒæŒ‰æ·»åŠ é¡ºåºï¼‰
    /// ```
    pub fn new_with_special_tokens(
        words: Vec<&str>,
        special_tokens: HashMap<String, usize>,
    ) -> Self {
        let mut encode = HashMap::new();
        let mut decode = HashMap::new();

        // Add special tokens first with their predefined IDs
        println!("\n=== åˆå§‹åŒ–è¯æ±‡è¡¨ï¼šæ·»åŠ ç‰¹æ®Šè¯å…ƒ ===");
        let mut special_tokens_sorted: Vec<_> = special_tokens.iter().collect();
        special_tokens_sorted.sort_by_key(|(_, id)| *id);

        for (token, id) in &special_tokens_sorted {
            encode.insert((*token).clone(), **id);
            decode.insert(**id, (*token).clone());
            println!("  âœ“ ç‰¹æ®Šè¯å…ƒ: '{}' -> ID {}", token, id);
        }
        println!("ç‰¹æ®Šè¯å…ƒæ·»åŠ å®Œæˆï¼Œå…± {} ä¸ª\n", special_tokens.len());

        // Add remaining words starting from the next available ID
        println!("=== æ·»åŠ å¸¸è§„è¯æ±‡ ===");
        let mut next_id = special_tokens.values().max().unwrap_or(&0) + 1;
        let mut added_count = 0;
        let mut skipped_count = 0;
        let mut chinese_count = 0;
        let mut english_count = 0;

        // ç»Ÿè®¡è¯å…ƒç±»å‹
        for word in words.iter() {
            let word_str = word.to_string();
            if !encode.contains_key(&word_str) {
                encode.insert(word_str.clone(), next_id);
                decode.insert(next_id, word_str.clone());

                // åˆ¤æ–­è¯å…ƒç±»å‹
                let is_chinese = word_str
                    .chars()
                    .any(|c| (c as u32) >= 0x4E00 && (c as u32) <= 0x9FFF);
                if is_chinese {
                    chinese_count += 1;
                    println!("  [ä¸­æ–‡] '{}' -> ID {}", word_str, next_id);
                } else {
                    english_count += 1;
                    println!("  [å…¶ä»–] '{}' -> ID {}", word_str, next_id);
                }

                added_count += 1;
                next_id += 1;
            } else {
                skipped_count += 1;
                println!(
                    "  [è·³è¿‡] '{}' (å·²å­˜åœ¨ï¼ŒID: {})",
                    word_str, encode[&word_str]
                );
            }
        }

        println!("\n=== è¯æ±‡è¡¨æ„å»ºå®Œæˆ ===");
        println!("  â€¢ æ–°å¢è¯å…ƒ: {} ä¸ª", added_count);
        println!("  â€¢ è·³è¿‡é‡å¤: {} ä¸ª", skipped_count);
        println!("  â€¢ ä¸­æ–‡è¯å…ƒ: {} ä¸ª", chinese_count);
        println!("  â€¢ å…¶ä»–è¯å…ƒ: {} ä¸ª", english_count);
        println!("  â€¢ æ€»è¯æ±‡é‡: {} ä¸ª", encode.len());
        println!("========================\n");

        let all_words: Vec<String> = decode.values().cloned().collect();

        Vocab {
            encode,
            decode,
            words: all_words,
            special_tokens,
        }
    }

    /// **ç¼–ç å•ä¸ªè¯**
    ///
    /// å°†è¯è½¬æ¢ä¸º token IDã€‚
    ///
    /// # å‚æ•°
    /// - `word`: è¦ç¼–ç çš„è¯
    ///
    /// # è¿”å›å€¼
    /// - `Some(usize)`: è¯å¯¹åº”çš„ ID
    /// - `None`: è¯ä¸åœ¨è¯æ±‡è¡¨ä¸­
    ///
    /// # ç¤ºä¾‹
    /// ```rust
    /// if let Some(id) = vocab.encode("ä½ å¥½") {
    ///     println!("'ä½ å¥½' çš„ ID æ˜¯: {}", id);
    /// }
    /// ```
    pub fn encode(&self, word: &str) -> Option<usize> {
        self.encode.get(word).copied()
    }

    /// **ç¼–ç å•ä¸ªè¯ï¼ˆå¸¦æœªçŸ¥è¯å›é€€ï¼‰**
    ///
    /// å°†è¯è½¬æ¢ä¸º token IDï¼Œå¦‚æœè¯ä¸åœ¨è¯æ±‡è¡¨ä¸­åˆ™è¿”å› `<|unk|>` çš„ IDã€‚
    ///
    /// # å‚æ•°
    /// - `word`: è¦ç¼–ç çš„è¯
    ///
    /// # è¿”å›å€¼
    /// è¯å¯¹åº”çš„ IDï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› `<|unk|>` çš„ ID (é€šå¸¸æ˜¯ 1)
    ///
    /// # ä¸ºä»€ä¹ˆéœ€è¦æœªçŸ¥è¯å¤„ç†ï¼Ÿ
    ///
    /// è®­ç»ƒåçš„è¯æ±‡è¡¨æ˜¯å›ºå®šçš„ï¼Œä½†æ¨ç†æ—¶å¯èƒ½é‡åˆ°æ–°è¯ï¼š
    /// - ç”Ÿåƒ»å­—ã€ä¸“æœ‰åè¯
    /// - æ–°å‡ºç°çš„ç½‘ç»œç”¨è¯­
    /// - æ‹¼å†™é”™è¯¯çš„è¯
    ///
    /// ä½¿ç”¨ `<|unk|>` è®©æ¨¡å‹èƒ½å¤Ÿå¤„ç†è¿™äº›æƒ…å†µï¼Œè€Œä¸æ˜¯å´©æºƒã€‚
    ///
    /// # ç¤ºä¾‹
    /// ```rust
    /// let id = vocab.encode_with_unk("ç«æ˜Ÿæ–‡è¯æ±‡"); // è¿”å› 1 (<|unk|>)
    /// ```
    pub fn encode_with_unk(&self, word: &str) -> usize {
        match self.encode(word) {
            Some(id) => id,
            None => *self.special_tokens.get("<|unk|>").unwrap_or(&0),
        }
    }

    /// **è§£ç å•ä¸ª token ID**
    ///
    /// å°† token ID è½¬æ¢å›å¯¹åº”çš„è¯ã€‚
    ///
    /// # å‚æ•°
    /// - `token_id`: è¦è§£ç çš„ token ID
    ///
    /// # è¿”å›å€¼
    /// - `Some(&String)`: ID å¯¹åº”çš„è¯
    /// - `None`: ID ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
    #[allow(dead_code)]
    pub fn decode(&self, token_id: usize) -> Option<&String> {
        self.decode.get(&token_id)
    }

    /// **ç¼–ç æ–‡æœ¬åºåˆ—**
    ///
    /// å°†æ•´æ®µæ–‡æœ¬è½¬æ¢ä¸º token ID åºåˆ—ï¼Œè¿™æ˜¯æ¨¡å‹è¾“å…¥çš„æ ‡å‡†æ ¼å¼ã€‚
    ///
    /// # ç®—æ³•æµç¨‹
    ///
    /// ```text
    /// 1. æ£€æµ‹è¯­è¨€ï¼ˆæ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦ 0x4E00-0x9FFFï¼‰
    ///
    /// 2a. å¦‚æœæ˜¯ä¸­æ–‡ï¼š
    ///     - ä½¿ç”¨ Jieba åˆ†è¯: "æˆ‘çˆ±ç¼–ç¨‹" â†’ ["æˆ‘", "çˆ±", "ç¼–ç¨‹"]
    ///     - æŸ¥è¡¨æ˜ å°„: ["æˆ‘", "çˆ±", "ç¼–ç¨‹"] â†’ [102, 358, 456]
    ///
    /// 2b. å¦‚æœæ˜¯è‹±æ–‡ï¼š
    ///     - æŒ‰ç©ºæ ¼åˆ†è¯: "I love coding" â†’ ["I", "love", "coding"]
    ///     - æŸ¥è¡¨æ˜ å°„: ["I", "love", "coding"] â†’ [78, 234, 567]
    ///
    /// 3. è¿”å› token ID åºåˆ—: [102, 358, 456]
    /// ```
    ///
    /// # å‚æ•°
    /// - `text`: è¦ç¼–ç çš„æ–‡æœ¬
    ///
    /// # è¿”å›å€¼
    /// token ID åºåˆ—
    ///
    /// # ç¤ºä¾‹
    /// ```rust
    /// let text = "æ·±åº¦å­¦ä¹ å¾ˆæœ‰è¶£";
    /// let token_ids = vocab.encode_sequence(text);
    /// // token_ids: [1234, 5678, 9012, 3456]
    /// ```
    pub fn encode_sequence(&self, text: &str) -> Vec<usize> {
        let mut tokens = Vec::new();

        // Check if the text contains Chinese characters
        let has_chinese = text
            .chars()
            .any(|c| (c as u32) >= 0x4E00 && (c as u32) <= 0x9FFF);

        if has_chinese {
            let jieba = jieba_instance(); // ä½¿ç”¨å…¨å±€å®ä¾‹
            let seg_list = jieba.cut(text, false);
            for word in seg_list {
                if !word.trim().is_empty() {
                    let token_id = self.encode_with_unk(word.trim());
                    tokens.push(token_id);
                }
            }
        } else {
            // For non-Chinese text, use simple tokenization
            for word in text.split_whitespace() {
                let token_id = self.encode_with_unk(word);
                tokens.push(token_id);
            }
        }

        tokens
    }

    /// **è§£ç  token ID åºåˆ—**
    ///
    /// å°† token ID åºåˆ—è½¬æ¢å›æ–‡æœ¬ï¼Œè¿™æ˜¯æ¨¡å‹è¾“å‡ºçš„æ ‡å‡†æ ¼å¼ã€‚
    ///
    /// # è§£ç æµç¨‹
    ///
    /// ```text
    /// è¾“å…¥: [102, 358, 1524]
    ///   â†“ æŸ¥è¡¨
    /// ["æˆ‘", "çˆ±", "äººå·¥æ™ºèƒ½"]
    ///   â†“ ç”¨ç©ºæ ¼è¿æ¥
    /// "æˆ‘ çˆ± äººå·¥æ™ºèƒ½"
    /// ```
    ///
    /// # æ³¨æ„äº‹é¡¹
    ///
    /// - **ç©ºæ ¼é—®é¢˜**ï¼šè§£ç åçš„ä¸­æ–‡è¯ä¹‹é—´ä¼šæœ‰ç©ºæ ¼
    /// - **åå¤„ç†**ï¼šéœ€è¦åœ¨ `llm.rs` ä¸­ç§»é™¤ä¸­æ–‡è¯ä¹‹é—´çš„ç©ºæ ¼
    /// - **ç‰¹æ®Šè¯å…ƒ**ï¼š`<|pad|>`, `<|unk|>` ç­‰ä¹Ÿä¼šè¢«è§£ç å‡ºæ¥
    ///
    /// # å‚æ•°
    /// - `token_ids`: token ID åºåˆ—
    ///
    /// # è¿”å›å€¼
    /// è§£ç åçš„æ–‡æœ¬ï¼ˆè¯ä¹‹é—´ç”¨ç©ºæ ¼åˆ†éš”ï¼‰
    ///
    /// # ç¤ºä¾‹
    /// ```rust
    /// let token_ids = vec![102, 358, 1524];
    /// let text = vocab.decode_sequence(&token_ids);
    /// // text: "æˆ‘ çˆ± äººå·¥æ™ºèƒ½"
    /// ```
    pub fn decode_sequence(&self, token_ids: &[usize]) -> String {
        let mut result = Vec::new();
        for &token_id in token_ids {
            if let Some(word) = self.decode.get(&token_id) {
                result.push(word.clone());
            }
        }
        result.join(" ")
    }

    /// **è·å–è¯æ±‡è¡¨å¤§å°**
    ///
    /// è¿”å›è¯æ±‡è¡¨ä¸­çš„è¯æ±‡æ€»æ•°ï¼ˆåŒ…æ‹¬ç‰¹æ®Šè¯å…ƒï¼‰ã€‚
    ///
    /// # è¿”å›å€¼
    /// è¯æ±‡é‡ï¼ˆé€šå¸¸åœ¨ 5,000 åˆ° 30,000 ä¹‹é—´ï¼‰
    pub fn len(&self) -> usize {
        self.encode.len()
    }

    /// **æ£€æŸ¥è¯æ±‡è¡¨æ˜¯å¦ä¸ºç©º**
    pub fn is_empty(&self) -> bool {
        self.encode.is_empty()
    }

    /// **è·å–æœªçŸ¥è¯çš„ token ID**
    ///
    /// è¿”å› `<|unk|>` çš„ IDï¼Œé€šå¸¸æ˜¯ 1ã€‚
    pub fn unk_token_id(&self) -> usize {
        *self.special_tokens.get("<|unk|>").unwrap_or(&0)
    }

    /// **è·å–å¡«å……è¯çš„ token ID**
    ///
    /// è¿”å› `<|pad|>` çš„ IDï¼Œé€šå¸¸æ˜¯ 0ã€‚
    pub fn pad_token_id(&self) -> usize {
        *self.special_tokens.get("<|pad|>").unwrap_or(&0)
    }

    /// **è·å–åºåˆ—ç»“æŸè¯çš„ token ID**
    ///
    /// è¿”å› `</s>` çš„ IDï¼Œé€šå¸¸æ˜¯ 3ã€‚
    pub fn eos_token_id(&self) -> usize {
        *self.special_tokens.get("</s>").unwrap_or(&0)
    }

    /// **è·å–åºåˆ—å¼€å§‹è¯çš„ token ID**
    ///
    /// è¿”å› `<|bos|>` çš„ IDï¼Œé€šå¸¸æ˜¯ 2ã€‚
    pub fn bos_token_id(&self) -> usize {
        *self.special_tokens.get("<|bos|>").unwrap_or(&0)
    }

    /// **é»˜è®¤è¯åˆ—è¡¨**
    ///
    /// è¿”å›ä¸€ä¸ªå°å‹è‹±æ–‡è¯æ±‡åˆ—è¡¨ï¼Œä¸»è¦ç”¨äºæµ‹è¯•ã€‚
    ///
    /// å®é™…è®­ç»ƒä¸­ï¼Œè¯æ±‡è¡¨ä¼šä»è®­ç»ƒæ•°æ®åŠ¨æ€æ„å»ºã€‚
    pub fn default_words() -> Vec<&'static str> {
        vec![
            "hello", "world", "this", "is", "rust", "the", "a", "an", "and", "or", "but", "in",
            "on", "at", "to", "for", "of", "with", "by", "from", "up", "about", "into", "through",
            "during", "before", "after", "above", "below", "between", "among", "as", "if", "when",
            "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other",
            "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very",
            "can", "will", "just", "don", "should", "now",
        ]
    }

    /// **é»˜è®¤ç‰¹æ®Šè¯å…ƒé…ç½®**
    ///
    /// è¿”å›æ ‡å‡†çš„ç‰¹æ®Šè¯å…ƒæ˜ å°„ï¼Œè¿™äº› ID æ˜¯å›ºå®šçš„ã€‚
    ///
    /// # ç‰¹æ®Šè¯å…ƒåˆ—è¡¨
    ///
    /// | è¯å…ƒ | ID | ç”¨é€” |
    /// |------|----|----|
    /// | `<|pad|>` | 0 | å¡«å……çŸ­åºåˆ— |
    /// | `<|unk|>` | 1 | æœªçŸ¥è¯å ä½ç¬¦ |
    /// | `<|bos|>` | 2 | å¼€å§‹æ ‡è®° |
    /// | `</s>` | 3 | ç»“æŸæ ‡è®° |
    /// | `<|sep|>` | 4 | åˆ†éš”ç¬¦ |
    /// | `<|cls|>` | 5 | åˆ†ç±»æ ‡è®° |
    /// | `<|mask|>` | 6 | æ©ç æ ‡è®° |
    pub fn default_special_tokens() -> HashMap<String, usize> {
        let mut special_tokens = HashMap::new();
        special_tokens.insert("<|pad|>".to_string(), 0);
        special_tokens.insert("<|unk|>".to_string(), 1);
        special_tokens.insert("<|bos|>".to_string(), 2);
        special_tokens.insert("</s>".to_string(), 3); // End of sequence
        special_tokens.insert("<|sep|>".to_string(), 4); // Separator
        special_tokens.insert("<|cls|>".to_string(), 5); // Classification
        special_tokens.insert("<|mask|>".to_string(), 6); // Masked token
        special_tokens
    }

    /// **ä»æ–‡æœ¬æ•°æ®æ„å»ºè¯æ±‡è¡¨**
    ///
    /// è¿™æ˜¯è¯æ±‡è¡¨æ„å»ºçš„ä¸»å…¥å£å‡½æ•°ï¼Œå¤„ç†è®­ç»ƒæ•°æ®å¹¶æå–æ‰€æœ‰å”¯ä¸€è¯å…ƒã€‚
    ///
    /// # å¤„ç†æµç¨‹
    ///
    /// ```text
    /// 1. åˆå§‹åŒ–è¯æ±‡é›†åˆï¼Œæ·»åŠ ç‰¹æ®Šè¯å…ƒ
    ///
    /// 2. å¯¹æ¯ä¸ªè®­ç»ƒæ–‡æœ¬ï¼š
    ///    a. æ£€æµ‹è¯­è¨€ï¼ˆä¸­æ–‡ vs è‹±æ–‡ï¼‰
    ///    b. ä¸­æ–‡ï¼šä½¿ç”¨ Jieba åˆ†è¯
    ///    c. è‹±æ–‡ï¼šæŒ‰ç©ºæ ¼åˆ†è¯
    ///    d. æå–æˆè¯­å’Œæœ‰æ„ä¹‰çš„çŸ­è¯­
    ///    e. æ·»åŠ åˆ°è¯æ±‡é›†åˆ
    ///
    /// 3. ç»Ÿè®¡å¹¶è¾“å‡ºï¼š
    ///    - ä¸­æ–‡è¯å…ƒæ•°é‡
    ///    - è‹±æ–‡è¯å…ƒæ•°é‡
    ///    - æœ€ç»ˆè¯æ±‡è¡¨å¤§å°
    ///
    /// 4. åˆ›å»ºè¯æ±‡è¡¨å®ä¾‹
    /// ```
    ///
    /// # å‚æ•°
    /// - `texts`: æ‰€æœ‰è®­ç»ƒæ–‡æœ¬ï¼ˆåŒ…æ‹¬é¢„è®­ç»ƒå’Œå¯¹è¯æ•°æ®ï¼‰
    /// - `vocab_set`: è¯æ±‡é›†åˆï¼ˆä¼šè¢«å°±åœ°ä¿®æ”¹ï¼‰
    ///
    /// # æ€§èƒ½è€ƒè™‘
    ///
    /// - ä½¿ç”¨ `HashSet` è‡ªåŠ¨å»é‡ï¼ŒO(1) æ’å…¥å’ŒæŸ¥æ‰¾
    /// - Jieba åˆ†è¯è¾ƒæ…¢ï¼Œä½†åªåœ¨æ„å»ºé˜¶æ®µè¿è¡Œä¸€æ¬¡
    /// - æˆè¯­è¯†åˆ«ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ€§èƒ½è‰¯å¥½
    ///
    /// # ç¤ºä¾‹è¾“å‡º
    ///
    /// ```text
    /// ğŸ”§ å¼€å§‹å¤„ç†æ–‡æœ¬æ•°æ®ä»¥æ„å»ºè¯æ±‡è¡¨...
    ///   ğŸ“Š å¾…å¤„ç†æ–‡æœ¬æ•°é‡: 1000
    ///   âœ“ å·²æ·»åŠ  7 ä¸ªç‰¹æ®Šè¯å…ƒ
    ///
    /// ğŸ“ å¼€å§‹åˆ†è¯å¤„ç†...
    ///   ğŸ“„ å¤„ç†æ–‡æœ¬ [1/1000]
    ///      å†…å®¹é¢„è§ˆ: æ·±åº¦å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯...
    ///      ç±»å‹: ä¸­æ–‡æ–‡æœ¬
    ///      â³ å¼€å§‹ Jieba åˆ†è¯...
    ///      âœ“ åˆ†è¯å®Œæˆï¼Œæå–äº† 15 ä¸ªè¯å…ƒ
    ///
    /// âœ… æ–‡æœ¬å¤„ç†å®Œæˆï¼
    /// ğŸ“Š åˆ†è¯å¤„ç†ç»Ÿè®¡:
    ///   â€¢ å¤„ç†æ–‡æœ¬æ€»æ•°: 1000 ä¸ª
    ///   â€¢ ä¸­æ–‡æ–‡æœ¬: 950 ä¸ª
    ///   â€¢ å…¶ä»–æ–‡æœ¬: 50 ä¸ª
    ///   â€¢ æœ€ç»ˆè¯æ±‡é›†å¤§å°: 12500 ä¸ªå”¯ä¸€è¯å…ƒ
    /// ```
    pub fn process_text_for_vocab(texts: &[String], vocab_set: &mut HashSet<String>) {
        use std::io::Write;

        println!("\nğŸ”§ å¼€å§‹å¤„ç†æ–‡æœ¬æ•°æ®ä»¥æ„å»ºè¯æ±‡è¡¨...");
        println!("  ğŸ“Š å¾…å¤„ç†æ–‡æœ¬æ•°é‡: {}", texts.len());

        let mut vocab_log = String::new();
        let mut idiom_writer = std::io::BufWriter::new(std::io::sink());

        vocab_set.insert("<|pad|>".to_string());
        vocab_set.insert("<|unk|>".to_string());
        vocab_set.insert("<|bos|>".to_string());
        vocab_set.insert("</s>".to_string());
        vocab_set.insert("<|sep|>".to_string());
        vocab_set.insert("<|cls|>".to_string());
        vocab_set.insert("<|mask|>".to_string());

        vocab_log.push_str("Initialized special tokens.\n");
        println!("  âœ“ å·²æ·»åŠ  7 ä¸ªç‰¹æ®Šè¯å…ƒ");

        // ä½¿ç”¨å…¨å±€ Jieba å®ä¾‹ï¼ˆå¦‚æœæœªåˆå§‹åŒ–ä¼šè‡ªåŠ¨åˆå§‹åŒ–ï¼‰
        println!("\nğŸ“ å¼€å§‹åˆ†è¯å¤„ç†...");
        let jieba = jieba_instance();

        // Process all training examples for vocabulary
        let total_texts = texts.len();
        let mut chinese_texts = 0;
        let mut english_texts = 0;

        for (idx, text) in texts.iter().enumerate() {
            // æ˜¾ç¤ºå½“å‰å¤„ç†çš„æ–‡æœ¬è¿›åº¦
            println!("\n  ğŸ“„ å¤„ç†æ–‡æœ¬ [{}/{}]", idx + 1, total_texts);

            // å®‰å…¨åœ°æˆªå–æ–‡æœ¬é¢„è§ˆï¼ˆå¤„ç† UTF-8 å­—ç¬¦è¾¹ç•Œï¼‰
            let preview = if text.len() > 50 {
                // ä½¿ç”¨å­—ç¬¦è¿­ä»£å™¨ç¡®ä¿ä¸ä¼šåœ¨å­—ç¬¦ä¸­é—´åˆ‡å‰²
                text.chars().take(50).collect::<String>()
            } else {
                text.clone()
            };
            println!("     å†…å®¹é¢„è§ˆ: {}...", preview);
            if let Err(e) = std::io::stdout().flush() {
                log::warn!("åˆ·æ–°æ ‡å‡†è¾“å‡ºå¤±è´¥: {}", e);
            }

            // Check if the text contains Chinese characters
            let has_chinese = text
                .chars()
                .any(|c| (c as u32) >= 0x4E00 && (c as u32) <= 0x9FFF);

            if has_chinese {
                chinese_texts += 1;
                println!("     ç±»å‹: ä¸­æ–‡æ–‡æœ¬");

                // Use Jieba for Chinese text tokenization
                println!("     â³ å¼€å§‹ Jieba åˆ†è¯...");
                if let Err(e) = std::io::stdout().flush() {
                    log::warn!("åˆ·æ–°æ ‡å‡†è¾“å‡ºå¤±è´¥: {}", e);
                }

                let tokens = jieba.cut(text, false);
                let token_count = tokens.len();

                println!("     âœ“ åˆ†è¯å®Œæˆï¼Œæå–äº† {} ä¸ªè¯å…ƒ", token_count);

                for token in tokens {
                    if !token.trim().is_empty() {
                        let token_trimmed = token.trim().to_string();
                        vocab_log.push_str(&format!("Token: {}\n", token_trimmed));
                        let is_new = vocab_set.insert(token_trimmed.clone());
                        if is_new {
                            println!("       + æ–°è¯å…ƒ: '{}'", token_trimmed);
                        }
                    }
                }

                // Process common Chinese idioms and phrases that might be missed by Jieba
                println!("     ğŸ” æå–æˆè¯­å’ŒçŸ­è¯­...");
                Self::extract_chinese_phrases(text, vocab_set);
            } else {
                english_texts += 1;
                println!("     ç±»å‹: è‹±æ–‡/å…¶ä»–æ–‡æœ¬");

                // Use the original method for non-Chinese text
                for word in text.split_whitespace() {
                    // Handle punctuation by splitting it from words
                    let mut current = String::new();
                    for c in word.chars() {
                        if c.is_ascii_punctuation() {
                            if !current.is_empty() {
                                vocab_log.push_str(&format!("Word: {}\n", current));
                                let is_new = vocab_set.insert(current.clone());
                                if is_new {
                                    println!("       + æ–°è¯å…ƒ: '{}'", current);
                                }
                                current.clear();
                            }
                            vocab_log.push_str(&format!("Punctuation: {}\n", c));
                            let is_new = vocab_set.insert(c.to_string());
                            if is_new {
                                println!("       + æ–°æ ‡ç‚¹: '{}'", c);
                            }
                        } else {
                            current.push(c);
                        }
                    }
                    if !current.is_empty() {
                        vocab_log.push_str(&format!("Word: {}\n", current));
                        let is_new = vocab_set.insert(current.clone());
                        if is_new {
                            println!("       + æ–°è¯å…ƒ: '{}'", current);
                        }
                    }
                }
            }

            println!("     ğŸ“Š å½“å‰è¯æ±‡è¡¨å¤§å°: {} ä¸ªå”¯ä¸€è¯å…ƒ", vocab_set.len());
        }

        // æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        println!("\nâœ… æ–‡æœ¬å¤„ç†å®Œæˆï¼");
        println!("\nğŸ“Š åˆ†è¯å¤„ç†ç»Ÿè®¡:");
        println!("  â€¢ å¤„ç†æ–‡æœ¬æ€»æ•°: {} ä¸ª", total_texts);
        println!("  â€¢ ä¸­æ–‡æ–‡æœ¬: {} ä¸ª", chinese_texts);
        println!("  â€¢ å…¶ä»–æ–‡æœ¬: {} ä¸ª", english_texts);
        println!("  â€¢ æœ€ç»ˆè¯æ±‡é›†å¤§å°: {} ä¸ªå”¯ä¸€è¯å…ƒ", vocab_set.len());

        let _ = writeln!(idiom_writer, "{}", vocab_log);
    }

    /// **æå–ä¸­æ–‡çŸ­è¯­å’Œæˆè¯­**
    ///
    /// ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¯†åˆ«ä¸­æ–‡æˆè¯­å’Œæœ‰æ„ä¹‰çš„çŸ­è¯­ã€‚
    ///
    /// # è¯†åˆ«ç­–ç•¥
    ///
    /// 1. **å››å­—æˆè¯­æ£€æµ‹**ï¼š
    ///    - æ­£åˆ™: `[\u4e00-\u9fff]{4}` åŒ¹é…4ä¸ªè¿ç»­ä¸­æ–‡å­—ç¬¦
    ///    - éªŒè¯: åœ¨æˆè¯­å­—å…¸ä¸­æŸ¥æ‰¾ï¼ˆ`data/chinese_idioms.json`ï¼‰
    ///    - ç¤ºä¾‹: "ä¸€å¸†é£é¡º", "ç”»é¾™ç‚¹ç›"
    ///
    /// 2. **æœ‰æ„ä¹‰çŸ­è¯­æ£€æµ‹**ï¼š
    ///    - æ­£åˆ™: `[\u4e00-\u9fff]{2,6}` åŒ¹é…2-6ä¸ªä¸­æ–‡å­—ç¬¦
    ///    - éªŒè¯: ä½¿ç”¨ Jieba æ£€æŸ¥æ˜¯å¦ä¸ºç‹¬ç«‹è¯ç»„
    ///    - ç¤ºä¾‹: "äººå·¥æ™ºèƒ½", "æ·±åº¦å­¦ä¹ "
    ///
    /// # ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªï¼Ÿ
    ///
    /// Jieba æœ‰æ—¶ä¼šæŠŠæˆè¯­æ‹†åˆ†æˆå•å­—ï¼Œå¯¼è‡´è¯­ä¹‰ä¸¢å¤±ï¼š
    /// ```text
    /// é”™è¯¯: "ä¸€å¸†é£é¡º" â†’ ["ä¸€", "å¸†", "é£", "é¡º"]
    /// æ­£ç¡®: "ä¸€å¸†é£é¡º" â†’ ["ä¸€å¸†é£é¡º"]
    /// ```
    ///
    /// # å‚æ•°
    /// - `text`: å¾…å¤„ç†çš„æ–‡æœ¬
    /// - `vocab_set`: è¯æ±‡é›†åˆï¼ˆä¼šæ·»åŠ è¯†åˆ«å‡ºçš„çŸ­è¯­ï¼‰
    fn extract_chinese_phrases(text: &str, vocab_set: &mut HashSet<String>) {
        // Common Chinese idioms (å››å­—æˆè¯­) - these are often not segmented properly by Jieba
        let idiom_regex = match Regex::new(r"[\u4e00-\u9fff]{4}") {
            Ok(re) => re,
            Err(e) => {
                log::warn!("æˆè¯­æ­£åˆ™ç¼–è¯‘å¤±è´¥: {}ï¼Œè·³è¿‡æˆè¯­æå–", e);
                return;
            }
        };
        for mat in idiom_regex.find_iter(text) {
            let idiom = mat.as_str();
            if Self::is_common_chinese_idiom(idiom) {
                vocab_set.insert(idiom.to_string());
            }
        }

        // Common multi-character phrases that might be relevant
        let phrase_regex = match Regex::new(r"[\u4e00-\u9fff]{2,6}") {
            Ok(re) => re,
            Err(e) => {
                log::warn!("çŸ­è¯­æ­£åˆ™ç¼–è¯‘å¤±è´¥: {}ï¼Œè·³è¿‡çŸ­è¯­æå–", e);
                return;
            }
        };
        for mat in phrase_regex.find_iter(text) {
            let phrase = mat.as_str();
            if Self::is_meaningful_phrase(phrase) {
                vocab_set.insert(phrase.to_string());
            }
        }
    }

    /// **æ£€æŸ¥æ˜¯å¦ä¸ºå¸¸è§ä¸­æ–‡æˆè¯­**
    ///
    /// éªŒè¯4å­—ç¬¦ä¸²æ˜¯å¦åœ¨æˆè¯­å­—å…¸ä¸­ã€‚
    ///
    /// # éªŒè¯è§„åˆ™
    ///
    /// 1. å¿…é¡»æ˜¯4ä¸ªå­—ç¬¦
    /// 2. æ‰€æœ‰å­—ç¬¦å¿…é¡»æ˜¯ä¸­æ–‡ï¼ˆ0x4E00-0x9FFFï¼‰
    /// 3. åœ¨åŠ è½½çš„æˆè¯­å­—å…¸ä¸­å­˜åœ¨
    ///
    /// # å‚æ•°
    /// - `idiom`: å¾…æ£€æŸ¥çš„å­—ç¬¦ä¸²
    ///
    /// # è¿”å›å€¼
    /// - `true`: æ˜¯å¸¸è§æˆè¯­
    /// - `false`: ä¸æ˜¯æˆè¯­æˆ–ä¸ç¬¦åˆè§„åˆ™
    fn is_common_chinese_idiom(idiom: &str) -> bool {
        let mut chars = idiom.chars();
        if chars.clone().count() != 4 {
            return false;
        }
        if !chars.all(|c| c.is_chinese()) {
            return false;
        }
        common_idioms().contains(idiom) // This works because HashSet<String> implements contains for &str
    }

    /// **æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ„ä¹‰çš„ä¸­æ–‡çŸ­è¯­**
    ///
    /// ä½¿ç”¨ Jieba åˆ†è¯ç»“æœåˆ¤æ–­çŸ­è¯­æ˜¯å¦ä¸ºç‹¬ç«‹çš„è¯­ä¹‰å•å…ƒã€‚
    ///
    /// # åˆ¤æ–­è§„åˆ™
    ///
    /// 1. é•¿åº¦åœ¨ 2-8 ä¸ªå­—ç¬¦ä¹‹é—´
    /// 2. æ‰€æœ‰å­—ç¬¦éƒ½æ˜¯ä¸­æ–‡
    /// 3. Jieba å°†å…¶è¯†åˆ«ä¸ºä¸€ä¸ªæˆ–ä¸¤ä¸ªè¯ç»„
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```text
    /// "äººå·¥æ™ºèƒ½" â†’ Jieba: ["äººå·¥æ™ºèƒ½"] â†’ 1ä¸ªè¯ â†’ true
    /// "æ·±åº¦å­¦ä¹ " â†’ Jieba: ["æ·±åº¦", "å­¦ä¹ "] â†’ 2ä¸ªè¯ï¼Œæ€»é•¿åº¦åŒ¹é… â†’ true
    /// "çš„æ˜¯åœ¨" â†’ Jieba: ["çš„", "æ˜¯", "åœ¨"] â†’ 3ä¸ªè¯ â†’ false
    /// ```
    ///
    /// # å‚æ•°
    /// - `phrase`: å¾…æ£€æŸ¥çš„çŸ­è¯­
    ///
    /// # è¿”å›å€¼
    /// - `true`: æœ‰æ„ä¹‰çš„çŸ­è¯­
    /// - `false`: ä¸æ˜¯æœ‰æ„ä¹‰çš„çŸ­è¯­
    fn is_meaningful_phrase(phrase: &str) -> bool {
        let length = phrase.chars().count();
        if length < 2 || length > 8 {
            return false;
        }
        if !phrase.chars().all(|c| c.is_chinese()) {
            return false;
        }
        if length == 4 && Self::is_common_chinese_idiom(phrase) {
            return true;
        }
        let jieba = jieba_instance(); // ä½¿ç”¨å…¨å±€å®ä¾‹
        let tokens = jieba.cut(phrase, false);
        if tokens.is_empty() {
            return false;
        }
        if tokens.len() == 1 {
            return true;
        }
        let total_len: usize = tokens.iter().map(|token| token.chars().count()).sum();
        total_len == length && tokens.len() <= 2
    }

    /// **ä»æ–‡æœ¬åˆ—è¡¨æ„å»ºè¯æ±‡è¡¨**
    ///
    /// é«˜çº§æ¥å£ï¼šå¤„ç†æ‰€æœ‰æ–‡æœ¬æ•°æ®å¹¶è¿”å›å®Œæ•´çš„è¯æ±‡è¡¨å®ä¾‹ã€‚
    ///
    /// # æ„å»ºæ­¥éª¤
    ///
    /// 1. åˆå§‹åŒ–ç©ºçš„è¯æ±‡é›†åˆ
    /// 2. è°ƒç”¨ `process_text_for_vocab` æå–æ‰€æœ‰è¯å…ƒ
    /// 3. æ’åºè¯æ±‡ï¼ˆç¡®ä¿ç¡®å®šæ€§é¡ºåºï¼‰
    /// 4. åˆ›å»ºè¯æ±‡è¡¨å®ä¾‹
    ///
    /// # å‚æ•°
    /// - `texts`: æ‰€æœ‰è®­ç»ƒæ–‡æœ¬
    ///
    /// # è¿”å›å€¼
    /// å®Œæ•´çš„è¯æ±‡è¡¨å®ä¾‹ï¼ŒåŒ…å«ï¼š
    /// - ç‰¹æ®Šè¯å…ƒï¼ˆID 0-6ï¼‰
    /// - ä»æ•°æ®ä¸­æå–çš„æ‰€æœ‰å”¯ä¸€è¯å…ƒï¼ˆID 7+ï¼‰
    ///
    /// # ä½¿ç”¨ç¤ºä¾‹
    ///
    /// ```rust
    /// let training_texts = load_training_data();
    /// let vocab = Vocab::build_from_texts(&training_texts);
    /// println!("è¯æ±‡è¡¨å¤§å°: {}", vocab.len());
    /// ```
    pub fn build_from_texts(texts: &[String]) -> Self {
        let mut vocab_set = HashSet::new();
        Self::process_text_for_vocab(texts, &mut vocab_set);

        let mut vocab_words: Vec<String> = vocab_set.into_iter().collect();
        vocab_words.sort(); // Sort for deterministic ordering

        // Create vectors of string references for the constructor
        let vocab_words_refs: Vec<&str> = vocab_words.iter().map(|s| s.as_str()).collect();
        let special_tokens = Self::default_special_tokens();

        Self::new_with_special_tokens(vocab_words_refs, special_tokens)
    }

    /// **ä¿å­˜è¯æ±‡è¡¨åˆ°æ–‡ä»¶**
    ///
    /// å°†è¯æ±‡è¡¨åºåˆ—åŒ–ä¸º JSON æ ¼å¼å¹¶ä¿å­˜åˆ°æ–‡ä»¶ã€‚
    ///
    /// # å‚æ•°
    /// - `path`: ä¿å­˜è·¯å¾„
    ///
    /// # è¿”å›å€¼
    /// - `Ok(())`: ä¿å­˜æˆåŠŸ
    /// - `Err`: æ–‡ä»¶å†™å…¥é”™è¯¯
    ///
    /// # ä½¿ç”¨åœºæ™¯
    ///
    /// è®­ç»ƒå®Œæˆåä¿å­˜è¯æ±‡è¡¨ï¼Œæ¨ç†æ—¶å¯ä»¥ç›´æ¥åŠ è½½ï¼Œé¿å…é‡æ–°æ„å»ºã€‚
    pub fn save_to_file<P: AsRef<Path>>(&self, path: P) -> std::io::Result<()> {
        let file = File::create(path)?;
        let mut writer = std::io::BufWriter::new(file);
        serde_json::to_writer(&mut writer, self)?;
        Ok(())
    }

    /// **ä»æ–‡ä»¶åŠ è½½è¯æ±‡è¡¨**
    ///
    /// ä» JSON æ–‡ä»¶ååºåˆ—åŒ–è¯æ±‡è¡¨ã€‚
    ///
    /// # å‚æ•°
    /// - `path`: è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„
    ///
    /// # è¿”å›å€¼
    /// - `Ok(Vocab)`: åŠ è½½æˆåŠŸçš„è¯æ±‡è¡¨
    /// - `Err`: æ–‡ä»¶è¯»å–æˆ– JSON è§£æé”™è¯¯
    ///
    /// # ä½¿ç”¨åœºæ™¯
    ///
    /// æ¨ç†æ—¶åŠ è½½å·²ä¿å­˜çš„è¯æ±‡è¡¨ï¼Œé¿å…é‡æ–°æ„å»ºï¼ˆèŠ‚çœæ—¶é—´ï¼‰ã€‚
    pub fn load_from_file<P: AsRef<Path>>(path: P) -> std::io::Result<Self> {
        let file = File::open(path)?;
        let reader = std::io::BufReader::new(file);
        let vocab: Vocab = serde_json::from_reader(reader)
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::InvalidData, e))?;
        Ok(vocab)
    }

    /// **åŠ¨æ€æ·»åŠ è¯æ±‡**
    ///
    /// åœ¨è¯æ±‡è¡¨åˆ›å»ºåæ·»åŠ æ–°è¯ã€‚
    ///
    /// # å‚æ•°
    /// - `word`: è¦æ·»åŠ çš„è¯
    ///
    /// # è¿”å›å€¼
    /// æ–°è¯çš„ IDï¼ˆå¦‚æœå·²å­˜åœ¨ï¼Œè¿”å›ç°æœ‰ IDï¼‰
    ///
    /// # æ³¨æ„
    ///
    /// åŠ¨æ€æ·»åŠ è¯ä¼šæ”¹å˜è¯æ±‡è¡¨å¤§å°ï¼Œå¯èƒ½å¯¼è‡´ï¼š
    /// - åµŒå…¥å±‚éœ€è¦æ‰©å±•
    /// - è¾“å‡ºæŠ•å½±å±‚éœ€è¦æ‰©å±•
    /// - æ¨¡å‹éœ€è¦é‡æ–°è®­ç»ƒ
    ///
    /// å› æ­¤ï¼Œä¸€èˆ¬åªåœ¨æ„å»ºé˜¶æ®µä½¿ç”¨ï¼Œè®­ç»ƒåä¸å»ºè®®æ·»åŠ æ–°è¯ã€‚
    pub fn add_word(&mut self, word: String) -> usize {
        if let Some(existing_id) = self.encode.get(&word) {
            return *existing_id;
        }

        let new_id = self.encode.len();
        self.encode.insert(word.clone(), new_id);
        self.decode.insert(new_id, word.clone());
        self.words.push(word);
        new_id
    }
}

/// **è¾…åŠ© traitï¼šæ£€æŸ¥å­—ç¬¦æ˜¯å¦ä¸ºä¸­æ–‡**
///
/// ä¸º `char` ç±»å‹æ·»åŠ  `is_chinese()` æ–¹æ³•ï¼Œç®€åŒ–ä¸­æ–‡å­—ç¬¦åˆ¤æ–­ã€‚
///
/// # Unicode èŒƒå›´
///
/// ä¸­æ–‡å­—ç¬¦çš„ Unicode èŒƒå›´æ˜¯ U+4E00 åˆ° U+9FFFï¼ˆCJK ç»Ÿä¸€è¡¨æ„æ–‡å­—ï¼‰ï¼š
/// - åŒ…å«å¸¸ç”¨ç®€ä½“å­—å’Œç¹ä½“å­—
/// - ä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šç¬¦å·
///
/// # ä½¿ç”¨ç¤ºä¾‹
///
/// ```rust
/// let c = 'ä¸­';
/// if c.is_chinese() {
///     println!("è¿™æ˜¯ä¸­æ–‡å­—ç¬¦");
/// }
/// ```
trait IsChinese {
    fn is_chinese(&self) -> bool;
}

impl IsChinese for char {
    fn is_chinese(&self) -> bool {
        (*self as u32) >= 0x4E00 && (*self as u32) <= 0x9FFF
    }
}

/// **å°†è¯æ±‡è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²**
///
/// ç”¨äºè°ƒè¯•å’Œæ—¥å¿—è¾“å‡ºï¼Œæ˜¾ç¤ºè¯æ±‡è¡¨çš„æ‰€æœ‰è¯åŠå…¶ IDã€‚
///
/// # æ ¼å¼
///
/// ```text
/// (0,<|pad|>),(1,<|unk|>),(2,<|bos|>),...
/// ```
impl From<Vocab> for String {
    fn from(val: Vocab) -> Self {
        String::from_iter(
            val.words
                .iter()
                .enumerate()
                .map(|(i, str)| format!("({i},{str}),")),
        )
    }
}

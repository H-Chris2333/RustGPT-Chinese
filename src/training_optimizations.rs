//!  è®­ç»ƒæ€§èƒ½ä¼˜åŒ–æ¨¡å—
//!
//!  åŒ…å«é˜¶æ®µ1çš„å¿«é€Ÿä¼˜åŒ–:
//!  1.  æ•°æ®é¢„å¤„ç†ç¼“å­˜
//!  2.  ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
//!  3.  æ—©åœæœºåˆ¶
//!  4.  è®­ç»ƒç›‘æ§å¢å¼º

use  ndarray::{Array1,  Array2};
use  crate::utils::softmax;
use  crate::llm::LLM;

impl  LLM  {
    ///  ä½¿ç”¨é¢„tokenizeçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    ///
    ///  è¿™ä¸ªæ–¹æ³•æ¥å—å·²ç»tokenizeçš„æ•°æ®ï¼Œé¿å…é‡å¤tokenization
    ///  ç›¸æ¯”trainæ–¹æ³•,åœ¨500ä¸ªepochçš„è®­ç»ƒä¸­å¯ä»¥èŠ‚çœ99.8%çš„tokenizationæ—¶é—´
    pub  fn  train_with_cached_tokens(
        &mut  self,
        tokenized_data:  Vec<Vec<usize>>,
        epochs:  usize,
        initial_lr:  f32,
    )  {
        self.set_training_mode(true);

        for  epoch  in  0..epochs  {
            let  decay_rate:  f32  =  0.95;
            let  decay_steps  =  10.0;
            let  current_lr  =  initial_lr  *  decay_rate.powf(epoch  as  f32  /  decay_steps);

            let  mut  total_loss  =  0.0;

            //  ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„tokenizedæ•°æ®ï¼Œæ— éœ€é‡å¤tokenize
            for  training_row  in  &tokenized_data  {
                if  training_row.len()  <  2  {
                    continue;
                }

                //  1.  Slice  input  and  targets
                let  input_ids  =  &training_row[..training_row.len()  -  1];
                let  target_ids  =  &training_row[1..];

                //  Forward  pass
                let  mut  input:  Array2<f32>  =  Array2::zeros((1,  input_ids.len()));
                input
                    .row_mut(0)
                    .assign(&input_ids.iter().map(|&x|  x  as  f32).collect::<Array1<f32>>());

                for  layer  in  &mut  self.network  {
                    input  =  layer.forward(&input);
                }

                let  logits  =  input;
                let  probs  =  softmax(&logits);
                total_loss  +=  Self::cross_entropy_loss_step(&probs,  target_ids);

                //  Backward  pass
                let  mut  grads_output  =  Self::compute_gradients_step(&probs,  target_ids);
                Self::clip_gradients(&mut  grads_output,  5.0);

                for  layer  in  self.network.iter_mut().rev()  {
                    grads_output  =  layer.backward(&grads_output,  current_lr);
                }
            }

            println!(
                "Epoch  {}:  Loss  =  {:.4},  LR  =  {:.6}",
                epoch,
                total_loss  /  tokenized_data.len()  as  f32,
                current_lr
            );
        }

        self.set_training_mode(false);
    }

    ///  ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦ï¼ˆå¸¦é‡å¯ï¼‰
    ///
    ///  #  å‚æ•°
    ///  -  `initial_lr`:  åˆå§‹å­¦ä¹ ç‡ï¼ˆå¦‚  0.001ï¼‰
    ///  -  `epoch`:  å½“å‰epoch
    ///  -  `total_epochs`:  æ€»epochæ•°
    ///  -  `num_restarts`:  é‡å¯æ¬¡æ•°ï¼ˆå¦‚2è¡¨ç¤ºè®­ç»ƒåˆ†ä¸º3ä¸ªå‘¨æœŸï¼‰
    ///
    ///  #  å…¬å¼
    ///  ```text
    ///  lr  =  lr_min  +  0.5  *  (lr_max  -  lr_min)  *  (1  +  cos(Ï€  *  progress))
    ///  ```
    ///  å…¶ä¸­  progress  =  (epoch  %  cycle_length)  /  cycle_length
    pub  fn  cosine_annealing_lr(
        initial_lr:  f32,
        epoch:  usize,
        total_epochs:  usize,
        num_restarts:  usize,
    )  ->  f32  {
        //  è®¡ç®—æ¯ä¸ªå‘¨æœŸçš„é•¿åº¦
        let  cycle_length  =  total_epochs  /  (num_restarts  +  1);

        //  å½“å‰åœ¨å‘¨æœŸå†…çš„ä½ç½®
        let  cycle_epoch  =  epoch  %  cycle_length;

        //  å‘¨æœŸå†…çš„è¿›åº¦  [0,  1]
        let  progress  =  cycle_epoch  as  f32  /  cycle_length  as  f32;

        //  æœ€å°å­¦ä¹ ç‡ä¸ºåˆå§‹å€¼çš„1%
        let  min_lr  =  initial_lr  *  0.01;

        //  ä½™å¼¦é€€ç«å…¬å¼
        min_lr  +  0.5  *  (initial_lr  -  min_lr)  *  (1.0  +  (std::f32::consts::PI  *  progress).cos())
    }

    ///  æ”¹è¿›çš„è®­ç»ƒæ–¹æ³•ï¼šä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡
    pub  fn  train_with_cosine_lr(
        &mut  self,
        tokenized_data:  Vec<Vec<usize>>,
        epochs:  usize,
        initial_lr:  f32,
        num_restarts:  usize,  //  æ¨èå€¼:  2-3
    )  {
        self.set_training_mode(true);

        for  epoch  in  0..epochs  {
            //  ğŸ”¥  ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡
            let  current_lr  =  Self::cosine_annealing_lr(initial_lr,  epoch,  epochs,  num_restarts);

            let  mut  total_loss  =  0.0;
            for  training_row  in  &tokenized_data  {
                if  training_row.len()  <  2  {
                    continue;
                }

                let  input_ids  =  &training_row[..training_row.len()  -  1];
                let  target_ids  =  &training_row[1..];

                let  mut  input:  Array2<f32>  =  Array2::zeros((1,  input_ids.len()));
                input
                    .row_mut(0)
                    .assign(&input_ids.iter().map(|&x|  x  as  f32).collect::<Array1<f32>>());

                for  layer  in  &mut  self.network  {
                    input  =  layer.forward(&input);
                }

                let  logits  =  input;
                let  probs  =  softmax(&logits);
                total_loss  +=  Self::cross_entropy_loss_step(&probs,  target_ids);

                let  mut  grads_output  =  Self::compute_gradients_step(&probs,  target_ids);
                Self::clip_gradients(&mut  grads_output,  5.0);

                for  layer  in  self.network.iter_mut().rev()  {
                    grads_output  =  layer.backward(&grads_output,  current_lr);
                }
            }

            //  æ¯10ä¸ªepochæ‰“å°ä¸€æ¬¡ï¼Œå‡å°‘è¾“å‡º
            if  epoch  %  10  ==  0  ||  epoch  ==  epochs  -  1  {
                println!(
                    "Epoch  {}:  Loss  =  {:.4},  LR  =  {:.6}",
                    epoch,
                    total_loss  /  tokenized_data.len()  as  f32,
                    current_lr
                );
            }
        }

        self.set_training_mode(false);
    }

    ///  å¸¦æ—©åœçš„è®­ç»ƒæ–¹æ³•
    ///
    ///  #  å‚æ•°
    ///  -  `patience`:  å®¹å¿å¤šå°‘ä¸ªepoch  lossä¸æ”¹å–„ï¼ˆæ¨è30-50ï¼‰
    ///
    ///  #  è¿”å›å€¼
    ///  è¿”å›å®é™…è®­ç»ƒçš„epochæ•°
    pub  fn  train_with_early_stopping(
        &mut  self,
        tokenized_data:  Vec<Vec<usize>>,
        max_epochs:  usize,
        initial_lr:  f32,
        patience:  usize,
    )  ->  usize  {
        self.set_training_mode(true);

        let  mut  best_loss  =  f32::INFINITY;
        let  mut  counter  =  0;
        let  min_delta  =  0.001f32;
        let  mut  best_epoch  =  0;

        for  epoch  in  0..max_epochs  {
            let  current_lr  =  Self::cosine_annealing_lr(initial_lr,  epoch,  max_epochs,  2);

            let  mut  total_loss  =  0.0;
            for  training_row  in  &tokenized_data  {
                if  training_row.len()  <  2  {
                    continue;
                }

                let  input_ids  =  &training_row[..training_row.len()  -  1];
                let  target_ids  =  &training_row[1..];

                let  mut  input:  Array2<f32>  =  Array2::zeros((1,  input_ids.len()));
                input
                    .row_mut(0)
                    .assign(&input_ids.iter().map(|&x|  x  as  f32).collect::<Array1<f32>>());

                for  layer  in  &mut  self.network  {
                    input  =  layer.forward(&input);
                }

                let  logits  =  input;
                let  probs  =  softmax(&logits);
                total_loss  +=  Self::cross_entropy_loss_step(&probs,  target_ids);

                let  mut  grads_output  =  Self::compute_gradients_step(&probs,  target_ids);
                Self::clip_gradients(&mut  grads_output,  5.0);

                for  layer  in  self.network.iter_mut().rev()  {
                    grads_output  =  layer.backward(&grads_output,  current_lr);
                }
            }

            let  avg_loss  =  total_loss  /  tokenized_data.len()  as  f32;

            if  epoch  %  10  ==  0  ||  epoch  ==  max_epochs  -  1  {
                println!(
                    "Epoch  {}:  Loss  =  {:.4},  LR  =  {:.6}",
                    epoch,  avg_loss,  current_lr
                );
            }

            //  ğŸ”¥  æ£€æŸ¥æ—©åœæ¡ä»¶
            if  avg_loss  <  best_loss  -  min_delta  {
                best_loss  =  avg_loss;
                best_epoch  =  epoch;
                counter  =  0;
            }  else  {
                counter  +=  1;
                if  counter  >=  patience  {
                    println!("\nğŸ›‘  æ—©åœè§¦å‘:");
                    println!("        â€¢  æœ€ä½³epoch:  {}",  best_epoch);
                    println!("        â€¢  æœ€ä½³loss:  {:.4}",  best_loss);
                    println!("        â€¢  åœæ­¢epoch:  {}",  epoch);
                    println!("        â€¢  èŠ‚çœæ—¶é—´:  {}  epochs\n",  max_epochs  -  epoch);

                    self.set_training_mode(false);
                    return  epoch  +  1;
                }
            }
        }

        self.set_training_mode(false);
        max_epochs
    }

    ///  è®¡ç®—æ¢¯åº¦L2èŒƒæ•°
    fn  compute_grad_norm(grads:  &Array2<f32>)  ->  f32  {
        grads.iter().map(|&x|  x  *  x).sum::<f32>().sqrt()
    }

    ///  å¸¦å®Œæ•´ç›‘æ§çš„è®­ç»ƒæ–¹æ³•ï¼ˆç»“åˆæ—©åœã€ä½™å¼¦å­¦ä¹ ç‡ã€è¯¦ç»†ç»Ÿè®¡ï¼‰
    ///
    ///  è¿™æ˜¯æœ€å®Œæ•´çš„è®­ç»ƒæ–¹æ³•ï¼Œæ¨èä½¿ç”¨
    pub  fn  train_monitored(
        &mut  self,
        tokenized_data:  Vec<Vec<usize>>,
        max_epochs:  usize,
        initial_lr:  f32,
        patience:  usize,
    )  ->  usize  {
        self.set_training_mode(true);

        let  mut  best_loss  =  f32::INFINITY;
        let  mut  counter  =  0;
        let  min_delta  =  0.001f32;
        let  mut  best_epoch  =  0;
        let  start_time  =  std::time::Instant::now();

        for  epoch  in  0..max_epochs  {
            let  epoch_start  =  std::time::Instant::now();
            let  current_lr  =  Self::cosine_annealing_lr(initial_lr,  epoch,  max_epochs,  2);

            let  mut  total_loss  =  0.0;
            let  mut  total_grad_norm  =  0.0;
            let  mut  sample_count  =  0;

            for  training_row  in  &tokenized_data  {
                if  training_row.len()  <  2  {
                    continue;
                }

                let  input_ids  =  &training_row[..training_row.len()  -  1];
                let  target_ids  =  &training_row[1..];

                let  mut  input:  Array2<f32>  =  Array2::zeros((1,  input_ids.len()));
                input
                    .row_mut(0)
                    .assign(&input_ids.iter().map(|&x|  x  as  f32).collect::<Array1<f32>>());

                for  layer  in  &mut  self.network  {
                    input  =  layer.forward(&input);
                }

                let  logits  =  input;
                let  probs  =  softmax(&logits);
                total_loss  +=  Self::cross_entropy_loss_step(&probs,  target_ids);

                let  mut  grads_output  =  Self::compute_gradients_step(&probs,  target_ids);

                //  è®°å½•æ¢¯åº¦èŒƒæ•°
                total_grad_norm  +=  Self::compute_grad_norm(&grads_output);

                Self::clip_gradients(&mut  grads_output,  5.0);

                for  layer  in  self.network.iter_mut().rev()  {
                    grads_output  =  layer.backward(&grads_output,  current_lr);
                }

                sample_count  +=  1;
            }

            let  epoch_time  =  epoch_start.elapsed().as_secs_f32();
            let  avg_loss  =  total_loss  /  sample_count  as  f32;
            let  avg_grad_norm  =  total_grad_norm  /  sample_count  as  f32;
            let  perplexity  =  avg_loss.exp();
            let  samples_per_sec  =  sample_count  as  f32  /  epoch_time;

            //  ğŸ“Š  ä¸°å¯Œçš„è®­ç»ƒä¿¡æ¯
            if  epoch  %  10  ==  0  ||  epoch  ==  max_epochs  -  1  {
                let  progress  =  (epoch  +  1)  as  f32  /  max_epochs  as  f32  *  100.0;
                let  elapsed  =  start_time.elapsed().as_secs();
                let  eta  =  (elapsed  as  f32  /  (epoch  +  1)  as  f32  *  (max_epochs  -  epoch  -  1)  as  f32)  as  u64;

                println!(
                    "[{:3}/{:3}]  ({:.1}%)  Loss:  {:.4}  |  PPL:  {:.2}  |  LR:  {:.6}  |  Grad:  {:.4}  |  Speed:  {:.1}  samples/s  |  ETA:  {}s",
                    epoch  +  1,
                    max_epochs,
                    progress,
                    avg_loss,
                    perplexity,
                    current_lr,
                    avg_grad_norm,
                    samples_per_sec,
                    eta
                );
            }

            //  ğŸ”¥  æ£€æŸ¥æ—©åœæ¡ä»¶
            if  avg_loss  <  best_loss  -  min_delta  {
                best_loss  =  avg_loss;
                best_epoch  =  epoch;
                counter  =  0;
            }  else  {
                counter  +=  1;
                if  counter  >=  patience  {
                    println!("\nğŸ›‘  æ—©åœè§¦å‘:");
                    println!("        â€¢  æœ€ä½³epoch:  {}",  best_epoch);
                    println!("        â€¢  æœ€ä½³loss:  {:.4}",  best_loss);
                    println!("        â€¢  åœæ­¢epoch:  {}",  epoch);
                    println!("        â€¢  èŠ‚çœæ—¶é—´:  {}  epochs\n",  max_epochs  -  epoch);

                    self.set_training_mode(false);
                    return  epoch  +  1;
                }
            }
        }

        self.set_training_mode(false);
        max_epochs
    }
}

//!  è®­ç»ƒæ€§èƒ½ä¼˜åŒ–æ¨¡å—
//!
//!  åŒ…å«é˜¶æ®µ1çš„å¿«é€Ÿä¼˜åŒ–:
//!  1.  æ•°æ®é¢„å¤„ç†ç¼“å­˜
//!  2.  ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
//!  3.  æ—©åœæœºåˆ¶
//!  4.  è®­ç»ƒç›‘æ§å¢å¼º
//!  5.  æ£€æŸ¥ç‚¹ç®¡ç†é›†æˆ

use ndarray::{Array1, Array2};
use crate::utils::softmax;
use crate::llm::LLM;

impl LLM {
    ///  ä½¿ç”¨é¢„tokenizeçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼Œç®€å•ç‰ˆï¼‰
    ///
    ///  è¿™ä¸ªæ–¹æ³•æ¥å—å·²ç»tokenizeçš„æ•°æ®ï¼Œé¿å…é‡å¤tokenization
    ///  ç›¸æ¯”trainæ–¹æ³•,åœ¨500ä¸ªepochçš„è®­ç»ƒä¸­å¯ä»¥èŠ‚çœ99.8%çš„tokenizationæ—¶é—´
    ///  æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸å¸¦æ—©åœå’Œæ£€æŸ¥ç‚¹ï¼Œä»…ç”¨äºå¿«é€Ÿæµ‹è¯•
    pub fn train_with_cached_tokens_simple(
        &mut self,
        tokenized_data: Vec<Vec<usize>>,
        epochs: usize,
        initial_lr: f32,
    ) {
        self.set_training_mode(true);

        for epoch in 0..epochs {
            let decay_rate: f32 = 0.95;
            let decay_steps = 10.0;
            let current_lr = initial_lr * decay_rate.powf(epoch as f32 / decay_steps);

            let mut total_loss = 0.0;

            //  ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„tokenizedæ•°æ®ï¼Œæ— éœ€é‡å¤tokenize
            for training_row in &tokenized_data {
                if training_row.len() < 2 {
                    continue;
                }

                //  1.  Slice  input  and  targets
                let input_ids = &training_row[..training_row.len() - 1];
                let target_ids = &training_row[1..];

                //  Forward  pass
                let mut input: Array2<f32> = Array2::zeros((1, input_ids.len()));
                input
                    .row_mut(0)
                    .assign(&input_ids.iter().map(|&x| x as f32).collect::<Array1<f32>>());

                for layer in &mut self.network {
                    input = layer.forward(&input);
                }

                let logits = input;
                let probs = softmax(&logits);
                total_loss += Self::cross_entropy_loss_step(&probs, target_ids);

                //  Backward  pass
                let mut grads_output = Self::compute_gradients_step(&probs, target_ids);
                Self::clip_gradients(&mut grads_output, 5.0);

                for layer in self.network.iter_mut().rev() {
                    grads_output = layer.backward(&grads_output, current_lr);
                }
            }

            println!(
                "Epoch  {}:  Loss  =  {:.4},  LR  =  {:.6}",
                epoch,
                total_loss / tokenized_data.len() as f32,
                current_lr
            );
        }

        self.set_training_mode(false);
    }

    ///  æ”¹è¿›çš„è®­ç»ƒæ–¹æ³•ï¼šä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡
    pub fn train_with_cosine_lr(
        &mut self,
        tokenized_data: Vec<Vec<usize>>,
        epochs: usize,
        initial_lr: f32,
        num_restarts: usize, //  æ¨èå€¼:  2-3
    ) {
        self.set_training_mode(true);

        for epoch in 0..epochs {
            //  ğŸ”¥  ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡
            let current_lr = Self::cosine_annealing_lr(initial_lr, epoch, epochs, num_restarts);

            let mut total_loss = 0.0;
            for training_row in &tokenized_data {
                if training_row.len() < 2 {
                    continue;
                }

                let input_ids = &training_row[..training_row.len() - 1];
                let target_ids = &training_row[1..];

                let mut input: Array2<f32> = Array2::zeros((1, input_ids.len()));
                input
                    .row_mut(0)
                    .assign(&input_ids.iter().map(|&x| x as f32).collect::<Array1<f32>>());

                for layer in &mut self.network {
                    input = layer.forward(&input);
                }

                let logits = input;
                let probs = softmax(&logits);
                total_loss += Self::cross_entropy_loss_step(&probs, target_ids);

                let mut grads_output = Self::compute_gradients_step(&probs, target_ids);
                Self::clip_gradients(&mut grads_output, 5.0);

                for layer in self.network.iter_mut().rev() {
                    grads_output = layer.backward(&grads_output, current_lr);
                }
            }

            //  æ¯10ä¸ªepochæ‰“å°ä¸€æ¬¡ï¼Œå‡å°‘è¾“å‡º
            if epoch % 10 == 0 || epoch == epochs - 1 {
                println!(
                    "Epoch  {}:  Loss  =  {:.4},  LR  =  {:.6}",
                    epoch,
                    total_loss / tokenized_data.len() as f32,
                    current_lr
                );
            }
        }

        self.set_training_mode(false);
    }

    ///  å¸¦æ—©åœçš„è®­ç»ƒæ–¹æ³•
    ///
    ///  #  å‚æ•°
    ///  -  `patience`:  å®¹å¿å¤šå°‘ä¸ªepoch  lossä¸æ”¹å–„ï¼ˆæ¨è30-50ï¼‰
    ///
    ///  #  è¿”å›å€¼
    ///  è¿”å›å®é™…è®­ç»ƒçš„epochæ•°
    pub fn train_with_early_stopping(
        &mut self,
        tokenized_data: Vec<Vec<usize>>,
        max_epochs: usize,
        initial_lr: f32,
        patience: usize,
    ) -> usize {
        self.set_training_mode(true);

        let mut best_loss = f32::INFINITY;
        let mut counter = 0;
        let min_delta = 0.001f32;
        let mut best_epoch = 0;

        for epoch in 0..max_epochs {
            let current_lr = Self::cosine_annealing_lr(initial_lr, epoch, max_epochs, 2);

            let mut total_loss = 0.0;
            for training_row in &tokenized_data {
                if training_row.len() < 2 {
                    continue;
                }

                let input_ids = &training_row[..training_row.len() - 1];
                let target_ids = &training_row[1..];

                let mut input: Array2<f32> = Array2::zeros((1, input_ids.len()));
                input
                    .row_mut(0)
                    .assign(&input_ids.iter().map(|&x| x as f32).collect::<Array1<f32>>());

                for layer in &mut self.network {
                    input = layer.forward(&input);
                }

                let logits = input;
                let probs = softmax(&logits);
                total_loss += Self::cross_entropy_loss_step(&probs, target_ids);

                let mut grads_output = Self::compute_gradients_step(&probs, target_ids);
                Self::clip_gradients(&mut grads_output, 5.0);

                for layer in self.network.iter_mut().rev() {
                    grads_output = layer.backward(&grads_output, current_lr);
                }
            }

            let avg_loss = total_loss / tokenized_data.len() as f32;

            if epoch % 10 == 0 || epoch == max_epochs - 1 {
                println!(
                    "Epoch  {}:  Loss  =  {:.4},  LR  =  {:.6}",
                    epoch, avg_loss, current_lr
                );
            }

            //  ğŸ”¥  æ£€æŸ¥æ—©åœæ¡ä»¶
            if avg_loss < best_loss - min_delta {
                best_loss = avg_loss;
                best_epoch = epoch;
                counter = 0;
            } else {
                counter += 1;
                if counter >= patience {
                    println!("\nğŸ›‘  æ—©åœè§¦å‘:");
                    println!("        â€¢  æœ€ä½³epoch:  {}", best_epoch);
                    println!("        â€¢  æœ€ä½³loss:  {:.4}", best_loss);
                    println!("        â€¢  åœæ­¢epoch:  {}", epoch);
                    println!("        â€¢  èŠ‚çœæ—¶é—´:  {}  epochs\n", max_epochs - epoch);

                    self.set_training_mode(false);
                    return epoch + 1;
                }
            }
        }

        self.set_training_mode(false);
        max_epochs
    }

    ///  å¸¦å®Œæ•´ç›‘æ§çš„è®­ç»ƒæ–¹æ³•ï¼ˆç»“åˆæ—©åœã€ä½™å¼¦å­¦ä¹ ç‡ã€è¯¦ç»†ç»Ÿè®¡ï¼‰
    ///
    ///  è¿™æ˜¯å®Œæ•´çš„è®­ç»ƒæ–¹æ³•ï¼Œä½¿ç”¨é¢„tokenizedæ•°æ®
    ///  æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•ä¸llm.rsä¸­çš„train_monitoredä¸åŒï¼Œä½¿ç”¨é¢„tokenizedæ•°æ®é¿å…é‡å¤tokenization
    pub fn train_monitored_tokenized(
        &mut self,
        tokenized_data: Vec<Vec<usize>>,
        max_epochs: usize,
        initial_lr: f32,
        patience: usize,
    ) -> usize {
        self.set_training_mode(true);

        let mut best_loss = f32::INFINITY;
        let mut counter = 0;
        let min_delta = 0.001f32;
        let mut best_epoch = 0;
        let start_time = std::time::Instant::now();

        for epoch in 0..max_epochs {
            let epoch_start = std::time::Instant::now();
            let current_lr = Self::cosine_annealing_lr(initial_lr, epoch, max_epochs, 2);

            let mut total_loss = 0.0;
            let mut total_grad_norm = 0.0;
            let mut sample_count = 0;

            for training_row in &tokenized_data {
                if training_row.len() < 2 {
                    continue;
                }

                let input_ids = &training_row[..training_row.len() - 1];
                let target_ids = &training_row[1..];

                let mut input: Array2<f32> = Array2::zeros((1, input_ids.len()));
                input
                    .row_mut(0)
                    .assign(&input_ids.iter().map(|&x| x as f32).collect::<Array1<f32>>());

                for layer in &mut self.network {
                    input = layer.forward(&input);
                }

                let logits = input;
                let probs = softmax(&logits);
                total_loss += Self::cross_entropy_loss_step(&probs, target_ids);

                let mut grads_output = Self::compute_gradients_step(&probs, target_ids);

                //  è®°å½•æ¢¯åº¦èŒƒæ•°
                total_grad_norm += Self::compute_grad_norm(&grads_output);

                Self::clip_gradients(&mut grads_output, 5.0);

                for layer in self.network.iter_mut().rev() {
                    grads_output = layer.backward(&grads_output, current_lr);
                }

                sample_count += 1;
            }

            let epoch_time = epoch_start.elapsed().as_secs_f32();
            let avg_loss = total_loss / sample_count as f32;
            let avg_grad_norm = total_grad_norm / sample_count as f32;
            let perplexity = avg_loss.exp();
            let samples_per_sec = sample_count as f32 / epoch_time;

            //  ğŸ“Š  ä¸°å¯Œçš„è®­ç»ƒä¿¡æ¯
            if epoch % 10 == 0 || epoch == max_epochs - 1 {
                let progress = (epoch + 1) as f32 / max_epochs as f32 * 100.0;
                let elapsed = start_time.elapsed().as_secs();
                let eta =
                    (elapsed as f32 / (epoch + 1) as f32 * (max_epochs - epoch - 1) as f32) as u64;

                println!(
                    "[{:3}/{:3}]  ({:.1}%)  Loss:  {:.4}  |  PPL:  {:.2}  |  LR:  {:.6}  |  Grad:  {:.4}  |  Speed:  {:.1}  samples/s  |  ETA:  {}s",
                    epoch + 1,
                    max_epochs,
                    progress,
                    avg_loss,
                    perplexity,
                    current_lr,
                    avg_grad_norm,
                    samples_per_sec,
                    eta
                );
            }

            //  ğŸ”¥  æ£€æŸ¥æ—©åœæ¡ä»¶
            if avg_loss < best_loss - min_delta {
                best_loss = avg_loss;
                best_epoch = epoch;
                counter = 0;
            } else {
                counter += 1;
                if counter >= patience {
                    println!("\nğŸ›‘  æ—©åœè§¦å‘:");
                    println!("        â€¢  æœ€ä½³epoch:  {}", best_epoch);
                    println!("        â€¢  æœ€ä½³loss:  {:.4}", best_loss);
                    println!("        â€¢  åœæ­¢epoch:  {}", epoch);
                    println!("        â€¢  èŠ‚çœæ—¶é—´:  {}  epochs\n", max_epochs - epoch);

                    self.set_training_mode(false);
                    return epoch + 1;
                }
            }
        }

        self.set_training_mode(false);
        max_epochs
    }

    ///  å¸¦æ£€æŸ¥ç‚¹ç®¡ç†çš„è®­ç»ƒæ–¹æ³•
    ///
    ///  è¿™æ˜¯æœ€å®Œæ•´çš„è®­ç»ƒæ–¹æ³•ï¼Œé›†æˆäº†æ—©åœã€ä½™å¼¦å­¦ä¹ ç‡ã€æ£€æŸ¥ç‚¹ç®¡ç†
    ///
    ///  #  å‚æ•°
    ///  -  `checkpoint_manager`:  æ£€æŸ¥ç‚¹ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
    ///  -  `phase`:  è®­ç»ƒé˜¶æ®µæ ‡è¯†ï¼ˆå¦‚"pretraining", "instruction_tuning"ï¼‰
    ///  -  `resume_epoch`:  ä»å“ªä¸ªepochå¼€å§‹ï¼ˆç”¨äºresumeè®­ç»ƒï¼‰
    ///
    ///  #  è¿”å›å€¼
    ///  è¿”å›å®é™…è®­ç»ƒçš„epochæ•°
    pub fn train_with_checkpointing(
        &mut self,
        tokenized_data: Vec<Vec<usize>>,
        max_epochs: usize,
        initial_lr: f32,
        patience: usize,
        mut checkpoint_manager: Option<&mut crate::checkpoint_manager::CheckpointManager>,
        phase: &str,
        resume_epoch: usize,
    ) -> usize {
        self.set_training_mode(true);

        let mut best_loss = if let Some(ref manager) = checkpoint_manager {
            manager.get_best_loss()
        } else {
            f32::INFINITY
        };
        let mut counter = 0;
        let min_delta = 0.001f32;
        let mut best_epoch = resume_epoch;
        let start_time = std::time::Instant::now();

        for epoch in resume_epoch..max_epochs {
            let epoch_start = std::time::Instant::now();
            let current_lr = Self::cosine_annealing_lr(initial_lr, epoch, max_epochs, 2);

            let mut total_loss = 0.0;
            let mut total_grad_norm = 0.0;
            let mut sample_count = 0;

            for training_row in &tokenized_data {
                if training_row.len() < 2 {
                    continue;
                }

                let input_ids = &training_row[..training_row.len() - 1];
                let target_ids = &training_row[1..];

                let mut input: Array2<f32> = Array2::zeros((1, input_ids.len()));
                input
                    .row_mut(0)
                    .assign(&input_ids.iter().map(|&x| x as f32).collect::<Array1<f32>>());

                for layer in &mut self.network {
                    input = layer.forward(&input);
                }

                let logits = input;
                let probs = softmax(&logits);
                total_loss += Self::cross_entropy_loss_step(&probs, target_ids);

                let mut grads_output = Self::compute_gradients_step(&probs, target_ids);

                //  è®°å½•æ¢¯åº¦èŒƒæ•°
                total_grad_norm += Self::compute_grad_norm(&grads_output);

                Self::clip_gradients(&mut grads_output, 5.0);

                for layer in self.network.iter_mut().rev() {
                    grads_output = layer.backward(&grads_output, current_lr);
                }

                sample_count += 1;
            }

            let epoch_time = epoch_start.elapsed().as_secs_f32();
            let avg_loss = total_loss / sample_count as f32;
            let avg_grad_norm = total_grad_norm / sample_count as f32;
            let perplexity = avg_loss.exp();
            let samples_per_sec = sample_count as f32 / epoch_time;

            //  ğŸ“Š  ä¸°å¯Œçš„è®­ç»ƒä¿¡æ¯
            if epoch % 10 == 0 || epoch == max_epochs - 1 {
                let progress = (epoch + 1) as f32 / max_epochs as f32 * 100.0;
                let elapsed = start_time.elapsed().as_secs();
                let eta = (elapsed as f32 / (epoch - resume_epoch + 1) as f32
                    * (max_epochs - epoch - 1) as f32) as u64;

                println!(
                    "[{:3}/{:3}]  ({:.1}%)  Loss:  {:.4}  |  PPL:  {:.2}  |  LR:  {:.6}  |  Grad:  {:.4}  |  Speed:  {:.1}  samples/s  |  ETA:  {}s",
                    epoch + 1,
                    max_epochs,
                    progress,
                    avg_loss,
                    perplexity,
                    current_lr,
                    avg_grad_norm,
                    samples_per_sec,
                    eta
                );
            }

            //  ğŸ”¥  æ£€æŸ¥æ—©åœæ¡ä»¶å’Œä¿å­˜æ£€æŸ¥ç‚¹
            if avg_loss < best_loss - min_delta {
                best_loss = avg_loss;
                best_epoch = epoch;
                counter = 0;

                //  ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
                if let Some(ref mut manager) = checkpoint_manager {
                    manager.update_best_loss(avg_loss, epoch);

                    let metadata = crate::checkpoint_manager::CheckpointMetadata {
                        epoch,
                        loss: avg_loss,
                        learning_rate: current_lr,
                        timestamp: chrono::Local::now()
                            .format("%Y-%m-%d  %H:%M:%S")
                            .to_string(),
                        phase: phase.to_string(),
                    };

                    if let Err(e) = manager.save_checkpoint(self, metadata) {
                        log::warn!("ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥:  {}", e);
                    }
                }
            } else {
                counter += 1;
                if counter >= patience {
                    println!("\nğŸ›‘  æ—©åœè§¦å‘:");
                    println!("        â€¢  æœ€ä½³epoch:  {}", best_epoch);
                    println!("        â€¢  æœ€ä½³loss:  {:.4}", best_loss);
                    println!("        â€¢  åœæ­¢epoch:  {}", epoch);
                    println!("        â€¢  èŠ‚çœæ—¶é—´:  {}  epochs\n", max_epochs - epoch);

                    //  å°è¯•åŠ è½½æœ€ä½³æ£€æŸ¥ç‚¹
                    if let Some(ref manager) = checkpoint_manager {
                        if let Some(best_checkpoint_path) = manager.get_best_checkpoint() {
                            println!("ğŸ”„  åŠ è½½æœ€ä½³æ£€æŸ¥ç‚¹:  {}", best_checkpoint_path.display());
                            match crate::checkpoint_manager::CheckpointManager::load_checkpoint(
                                best_checkpoint_path,
                            ) {
                                Ok((best_llm, _metadata)) => {
                                    //  å¤åˆ¶æœ€ä½³æ¨¡å‹çš„å‚æ•°åˆ°å½“å‰æ¨¡å‹
                                    self.network = best_llm.network;
                                    println!("âœ…  å·²å›æ»šåˆ°æœ€ä½³epochçš„æ¨¡å‹å‚æ•°");
                                }
                                Err(e) => {
                                    log::warn!("åŠ è½½æœ€ä½³æ£€æŸ¥ç‚¹å¤±è´¥:  {}", e);
                                }
                            }
                        }
                    }

                    self.set_training_mode(false);
                    return epoch + 1;
                }
            }

            //  å‘¨æœŸæ€§ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if let Some(ref mut manager) = checkpoint_manager {
                if manager.should_save(epoch, avg_loss) && avg_loss >= best_loss {
                    let metadata = crate::checkpoint_manager::CheckpointMetadata {
                        epoch,
                        loss: avg_loss,
                        learning_rate: current_lr,
                        timestamp: chrono::Local::now()
                            .format("%Y-%m-%d  %H:%M:%S")
                            .to_string(),
                        phase: phase.to_string(),
                    };

                    if let Err(e) = manager.save_checkpoint(self, metadata) {
                        log::warn!("ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥:  {}", e);
                    }
                }
            }
        }

        self.set_training_mode(false);
        max_epochs
    }
}
